{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "metadata": {
        "id": "Uukn1XgBfxXT",
        "outputId": "b72cb511-4ad6-4717-efb5-bb061082ac94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__file__)\n",
        "!cat  /usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th4KhUGfZTAU",
        "outputId": "426efc80-14b8-4dcd-f584-444f7c376621"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\n",
            "\"\"\"\n",
            "Forest of trees-based ensemble methods.\n",
            "\n",
            "Those methods include random forests and extremely randomized trees.\n",
            "\n",
            "The module structure is the following:\n",
            "\n",
            "- The ``BaseForest`` base class implements a common ``fit`` method for all\n",
            "  the estimators in the module. The ``fit`` method of the base ``Forest``\n",
            "  class calls the ``fit`` method of each sub-estimator on random samples\n",
            "  (with replacement, a.k.a. bootstrap) of the training set.\n",
            "\n",
            "  The init of the sub-estimator is further delegated to the\n",
            "  ``BaseEnsemble`` constructor.\n",
            "\n",
            "- The ``ForestClassifier`` and ``ForestRegressor`` base classes further\n",
            "  implement the prediction logic by computing an average of the predicted\n",
            "  outcomes of the sub-estimators.\n",
            "\n",
            "- The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived\n",
            "  classes provide the user with concrete implementations of\n",
            "  the forest ensemble method using classical, deterministic\n",
            "  ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as\n",
            "  sub-estimator implementations.\n",
            "\n",
            "- The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived\n",
            "  classes provide the user with concrete implementations of the\n",
            "  forest ensemble method using the extremely randomized trees\n",
            "  ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as\n",
            "  sub-estimator implementations.\n",
            "\n",
            "Single and multi-output problems are both handled.\n",
            "\"\"\"\n",
            "\n",
            "# Authors: Gilles Louppe <g.louppe@gmail.com>\n",
            "#          Brian Holt <bdholt1@gmail.com>\n",
            "#          Joly Arnaud <arnaud.v.joly@gmail.com>\n",
            "#          Fares Hedayati <fares.hedayati@gmail.com>\n",
            "#\n",
            "# License: BSD 3 clause\n",
            "\n",
            "\n",
            "import threading\n",
            "from abc import ABCMeta, abstractmethod\n",
            "from numbers import Integral, Real\n",
            "from warnings import catch_warnings, simplefilter, warn\n",
            "\n",
            "import numpy as np\n",
            "from scipy.sparse import hstack as sparse_hstack\n",
            "from scipy.sparse import issparse\n",
            "\n",
            "from ..base import (\n",
            "    ClassifierMixin,\n",
            "    MultiOutputMixin,\n",
            "    RegressorMixin,\n",
            "    TransformerMixin,\n",
            "    _fit_context,\n",
            "    is_classifier,\n",
            ")\n",
            "from ..exceptions import DataConversionWarning\n",
            "from ..metrics import accuracy_score, r2_score\n",
            "from ..preprocessing import OneHotEncoder\n",
            "from ..tree import (\n",
            "    BaseDecisionTree,\n",
            "    DecisionTreeClassifier,\n",
            "    DecisionTreeRegressor,\n",
            "    ExtraTreeClassifier,\n",
            "    ExtraTreeRegressor,\n",
            ")\n",
            "from ..tree._tree import DOUBLE, DTYPE\n",
            "from ..utils import check_random_state, compute_sample_weight\n",
            "from ..utils._param_validation import Interval, RealNotInt, StrOptions\n",
            "from ..utils._tags import _safe_tags\n",
            "from ..utils.multiclass import check_classification_targets, type_of_target\n",
            "from ..utils.parallel import Parallel, delayed\n",
            "from ..utils.validation import (\n",
            "    _check_feature_names_in,\n",
            "    _check_sample_weight,\n",
            "    _num_samples,\n",
            "    check_is_fitted,\n",
            ")\n",
            "from ._base import BaseEnsemble, _partition_estimators\n",
            "\n",
            "__all__ = [\n",
            "    \"RandomForestClassifier\",\n",
            "    \"RandomForestRegressor\",\n",
            "    \"ExtraTreesClassifier\",\n",
            "    \"ExtraTreesRegressor\",\n",
            "    \"RandomTreesEmbedding\",\n",
            "]\n",
            "\n",
            "MAX_INT = np.iinfo(np.int32).max\n",
            "\n",
            "\n",
            "def _get_n_samples_bootstrap(n_samples, max_samples):\n",
            "    \"\"\"\n",
            "    Get the number of samples in a bootstrap sample.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_samples : int\n",
            "        Number of samples in the dataset.\n",
            "    max_samples : int or float\n",
            "        The maximum number of samples to draw from the total available:\n",
            "            - if float, this indicates a fraction of the total and should be\n",
            "              the interval `(0.0, 1.0]`;\n",
            "            - if int, this indicates the exact number of samples;\n",
            "            - if None, this indicates the total number of samples.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    n_samples_bootstrap : int\n",
            "        The total number of samples to draw for the bootstrap sample.\n",
            "    \"\"\"\n",
            "    if max_samples is None:\n",
            "        return n_samples\n",
            "\n",
            "    if isinstance(max_samples, Integral):\n",
            "        if max_samples > n_samples:\n",
            "            msg = \"`max_samples` must be <= n_samples={} but got value {}\"\n",
            "            raise ValueError(msg.format(n_samples, max_samples))\n",
            "        return max_samples\n",
            "\n",
            "    if isinstance(max_samples, Real):\n",
            "        return max(round(n_samples * max_samples), 1)\n",
            "\n",
            "\n",
            "def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):\n",
            "    \"\"\"\n",
            "    Private function used to _parallel_build_trees function.\"\"\"\n",
            "\n",
            "    random_instance = check_random_state(random_state)\n",
            "    sample_indices = random_instance.randint(\n",
            "        0, n_samples, n_samples_bootstrap, dtype=np.int32\n",
            "    )\n",
            "\n",
            "    return sample_indices\n",
            "\n",
            "\n",
            "def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):\n",
            "    \"\"\"\n",
            "    Private function used to forest._set_oob_score function.\"\"\"\n",
            "    sample_indices = _generate_sample_indices(\n",
            "        random_state, n_samples, n_samples_bootstrap\n",
            "    )\n",
            "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
            "    unsampled_mask = sample_counts == 0\n",
            "    indices_range = np.arange(n_samples)\n",
            "    unsampled_indices = indices_range[unsampled_mask]\n",
            "\n",
            "    return unsampled_indices\n",
            "\n",
            "\n",
            "def _parallel_build_trees(\n",
            "    tree,\n",
            "    bootstrap,\n",
            "    X,\n",
            "    y,\n",
            "    sample_weight,\n",
            "    tree_idx,\n",
            "    n_trees,\n",
            "    verbose=0,\n",
            "    class_weight=None,\n",
            "    n_samples_bootstrap=None,\n",
            "    missing_values_in_feature_mask=None,\n",
            "):\n",
            "    \"\"\"\n",
            "    Private function used to fit a single tree in parallel.\"\"\"\n",
            "    if verbose > 1:\n",
            "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
            "\n",
            "    if bootstrap:\n",
            "        n_samples = X.shape[0]\n",
            "        if sample_weight is None:\n",
            "            curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n",
            "        else:\n",
            "            curr_sample_weight = sample_weight.copy()\n",
            "\n",
            "        indices = _generate_sample_indices(\n",
            "            tree.random_state, n_samples, n_samples_bootstrap\n",
            "        )\n",
            "        sample_counts = np.bincount(indices, minlength=n_samples)\n",
            "        curr_sample_weight *= sample_counts\n",
            "\n",
            "        if class_weight == \"subsample\":\n",
            "            with catch_warnings():\n",
            "                simplefilter(\"ignore\", DeprecationWarning)\n",
            "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
            "        elif class_weight == \"balanced_subsample\":\n",
            "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
            "\n",
            "        tree._fit(\n",
            "            X,\n",
            "            y,\n",
            "            sample_weight=curr_sample_weight,\n",
            "            check_input=False,\n",
            "            missing_values_in_feature_mask=missing_values_in_feature_mask,\n",
            "        )\n",
            "    else:\n",
            "        tree._fit(\n",
            "            X,\n",
            "            y,\n",
            "            sample_weight=sample_weight,\n",
            "            check_input=False,\n",
            "            missing_values_in_feature_mask=missing_values_in_feature_mask,\n",
            "        )\n",
            "\n",
            "    return tree\n",
            "\n",
            "\n",
            "class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):\n",
            "    \"\"\"\n",
            "    Base class for forests of trees.\n",
            "\n",
            "    Warning: This class should not be used directly. Use derived classes\n",
            "    instead.\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n",
            "        \"bootstrap\": [\"boolean\"],\n",
            "        \"oob_score\": [\"boolean\", callable],\n",
            "        \"n_jobs\": [Integral, None],\n",
            "        \"random_state\": [\"random_state\"],\n",
            "        \"verbose\": [\"verbose\"],\n",
            "        \"warm_start\": [\"boolean\"],\n",
            "        \"max_samples\": [\n",
            "            None,\n",
            "            Interval(RealNotInt, 0.0, 1.0, closed=\"right\"),\n",
            "            Interval(Integral, 1, None, closed=\"left\"),\n",
            "        ],\n",
            "    }\n",
            "\n",
            "    @abstractmethod\n",
            "    def __init__(\n",
            "        self,\n",
            "        estimator,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        estimator_params=tuple(),\n",
            "        bootstrap=False,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        class_weight=None,\n",
            "        max_samples=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=estimator,\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=estimator_params,\n",
            "        )\n",
            "\n",
            "        self.bootstrap = bootstrap\n",
            "        self.oob_score = oob_score\n",
            "        self.n_jobs = n_jobs\n",
            "        self.random_state = random_state\n",
            "        self.verbose = verbose\n",
            "        self.warm_start = warm_start\n",
            "        self.class_weight = class_weight\n",
            "        self.max_samples = max_samples\n",
            "\n",
            "    def apply(self, X):\n",
            "        \"\"\"\n",
            "        Apply trees in the forest to X, return leaf indices.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X_leaves : ndarray of shape (n_samples, n_estimators)\n",
            "            For each datapoint x in X and for each tree in the forest,\n",
            "            return the index of the leaf x ends up in.\n",
            "        \"\"\"\n",
            "        X = self._validate_X_predict(X)\n",
            "        results = Parallel(\n",
            "            n_jobs=self.n_jobs,\n",
            "            verbose=self.verbose,\n",
            "            prefer=\"threads\",\n",
            "        )(delayed(tree.apply)(X, check_input=False) for tree in self.estimators_)\n",
            "\n",
            "        return np.array(results).T\n",
            "\n",
            "    def decision_path(self, X):\n",
            "        \"\"\"\n",
            "        Return the decision path in the forest.\n",
            "\n",
            "        .. versionadded:: 0.18\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            "            Return a node indicator matrix where non zero elements indicates\n",
            "            that the samples goes through the nodes. The matrix is of CSR\n",
            "            format.\n",
            "\n",
            "        n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
            "            The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
            "            gives the indicator value for the i-th estimator.\n",
            "        \"\"\"\n",
            "        X = self._validate_X_predict(X)\n",
            "        indicators = Parallel(\n",
            "            n_jobs=self.n_jobs,\n",
            "            verbose=self.verbose,\n",
            "            prefer=\"threads\",\n",
            "        )(\n",
            "            delayed(tree.decision_path)(X, check_input=False)\n",
            "            for tree in self.estimators_\n",
            "        )\n",
            "\n",
            "        n_nodes = [0]\n",
            "        n_nodes.extend([i.shape[1] for i in indicators])\n",
            "        n_nodes_ptr = np.array(n_nodes).cumsum()\n",
            "\n",
            "        return sparse_hstack(indicators).tocsr(), n_nodes_ptr\n",
            "\n",
            "    @_fit_context(prefer_skip_nested_validation=True)\n",
            "    def fit(self, X, y, sample_weight=None):\n",
            "        \"\"\"\n",
            "        Build a forest of trees from the training set (X, y).\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The training input samples. Internally, its dtype will be converted\n",
            "            to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csc_matrix``.\n",
            "\n",
            "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            "            The target values (class labels in classification, real numbers in\n",
            "            regression).\n",
            "\n",
            "        sample_weight : array-like of shape (n_samples,), default=None\n",
            "            Sample weights. If None, then samples are equally weighted. Splits\n",
            "            that would create child nodes with net zero or negative weight are\n",
            "            ignored while searching for a split in each node. In the case of\n",
            "            classification, splits are also ignored if they would result in any\n",
            "            single class carrying a negative weight in either child node.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        self : object\n",
            "            Fitted estimator.\n",
            "        \"\"\"\n",
            "        # Validate or convert input data\n",
            "        if issparse(y):\n",
            "            raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n",
            "\n",
            "        X, y = self._validate_data(\n",
            "            X,\n",
            "            y,\n",
            "            multi_output=True,\n",
            "            accept_sparse=\"csc\",\n",
            "            dtype=DTYPE,\n",
            "            force_all_finite=False,\n",
            "        )\n",
            "        # _compute_missing_values_in_feature_mask checks if X has missing values and\n",
            "        # will raise an error if the underlying tree base estimator can't handle missing\n",
            "        # values. Only the criterion is required to determine if the tree supports\n",
            "        # missing values.\n",
            "        estimator = type(self.estimator)(criterion=self.criterion)\n",
            "        missing_values_in_feature_mask = (\n",
            "            estimator._compute_missing_values_in_feature_mask(\n",
            "                X, estimator_name=self.__class__.__name__\n",
            "            )\n",
            "        )\n",
            "\n",
            "        if sample_weight is not None:\n",
            "            sample_weight = _check_sample_weight(sample_weight, X)\n",
            "\n",
            "        if issparse(X):\n",
            "            # Pre-sort indices to avoid that each individual tree of the\n",
            "            # ensemble sorts the indices.\n",
            "            X.sort_indices()\n",
            "\n",
            "        y = np.atleast_1d(y)\n",
            "        if y.ndim == 2 and y.shape[1] == 1:\n",
            "            warn(\n",
            "                (\n",
            "                    \"A column-vector y was passed when a 1d array was\"\n",
            "                    \" expected. Please change the shape of y to \"\n",
            "                    \"(n_samples,), for example using ravel().\"\n",
            "                ),\n",
            "                DataConversionWarning,\n",
            "                stacklevel=2,\n",
            "            )\n",
            "\n",
            "        if y.ndim == 1:\n",
            "            # reshape is necessary to preserve the data contiguity against vs\n",
            "            # [:, np.newaxis] that does not.\n",
            "            y = np.reshape(y, (-1, 1))\n",
            "\n",
            "        if self.criterion == \"poisson\":\n",
            "            if np.any(y < 0):\n",
            "                raise ValueError(\n",
            "                    \"Some value(s) of y are negative which is \"\n",
            "                    \"not allowed for Poisson regression.\"\n",
            "                )\n",
            "            if np.sum(y) <= 0:\n",
            "                raise ValueError(\n",
            "                    \"Sum of y is not strictly positive which \"\n",
            "                    \"is necessary for Poisson regression.\"\n",
            "                )\n",
            "\n",
            "        self._n_samples, self.n_outputs_ = y.shape\n",
            "\n",
            "        y, expanded_class_weight = self._validate_y_class_weight(y)\n",
            "\n",
            "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
            "            y = np.ascontiguousarray(y, dtype=DOUBLE)\n",
            "\n",
            "        if expanded_class_weight is not None:\n",
            "            if sample_weight is not None:\n",
            "                sample_weight = sample_weight * expanded_class_weight\n",
            "            else:\n",
            "                sample_weight = expanded_class_weight\n",
            "\n",
            "        if not self.bootstrap and self.max_samples is not None:\n",
            "            raise ValueError(\n",
            "                \"`max_sample` cannot be set if `bootstrap=False`. \"\n",
            "                \"Either switch to `bootstrap=True` or set \"\n",
            "                \"`max_sample=None`.\"\n",
            "            )\n",
            "        elif self.bootstrap:\n",
            "            n_samples_bootstrap = _get_n_samples_bootstrap(\n",
            "                n_samples=X.shape[0], max_samples=self.max_samples\n",
            "            )\n",
            "        else:\n",
            "            n_samples_bootstrap = None\n",
            "\n",
            "        self._n_samples_bootstrap = n_samples_bootstrap\n",
            "\n",
            "        self._validate_estimator()\n",
            "\n",
            "        if not self.bootstrap and self.oob_score:\n",
            "            raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
            "\n",
            "        random_state = check_random_state(self.random_state)\n",
            "\n",
            "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
            "            # Free allocated memory, if any\n",
            "            self.estimators_ = []\n",
            "\n",
            "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
            "\n",
            "        if n_more_estimators < 0:\n",
            "            raise ValueError(\n",
            "                \"n_estimators=%d must be larger or equal to \"\n",
            "                \"len(estimators_)=%d when warm_start==True\"\n",
            "                % (self.n_estimators, len(self.estimators_))\n",
            "            )\n",
            "\n",
            "        elif n_more_estimators == 0:\n",
            "            warn(\n",
            "                \"Warm-start fitting without increasing n_estimators does not \"\n",
            "                \"fit new trees.\"\n",
            "            )\n",
            "        else:\n",
            "            if self.warm_start and len(self.estimators_) > 0:\n",
            "                # We draw from the random state to get the random state we\n",
            "                # would have got if we hadn't used a warm_start.\n",
            "                random_state.randint(MAX_INT, size=len(self.estimators_))\n",
            "\n",
            "            trees = [\n",
            "                self._make_estimator(append=False, random_state=random_state)\n",
            "                for i in range(n_more_estimators)\n",
            "            ]\n",
            "\n",
            "            # Parallel loop: we prefer the threading backend as the Cython code\n",
            "            # for fitting the trees is internally releasing the Python GIL\n",
            "            # making threading more efficient than multiprocessing in\n",
            "            # that case. However, for joblib 0.12+ we respect any\n",
            "            # parallel_backend contexts set at a higher level,\n",
            "            # since correctness does not rely on using threads.\n",
            "            trees = Parallel(\n",
            "                n_jobs=self.n_jobs,\n",
            "                verbose=self.verbose,\n",
            "                prefer=\"threads\",\n",
            "            )(\n",
            "                delayed(_parallel_build_trees)(\n",
            "                    t,\n",
            "                    self.bootstrap,\n",
            "                    X,\n",
            "                    y,\n",
            "                    sample_weight,\n",
            "                    i,\n",
            "                    len(trees),\n",
            "                    verbose=self.verbose,\n",
            "                    class_weight=self.class_weight,\n",
            "                    n_samples_bootstrap=n_samples_bootstrap,\n",
            "                    missing_values_in_feature_mask=missing_values_in_feature_mask,\n",
            "                )\n",
            "                for i, t in enumerate(trees)\n",
            "            )\n",
            "\n",
            "            # Collect newly grown trees\n",
            "            self.estimators_.extend(trees)\n",
            "\n",
            "        if self.oob_score and (\n",
            "            n_more_estimators > 0 or not hasattr(self, \"oob_score_\")\n",
            "        ):\n",
            "            y_type = type_of_target(y)\n",
            "            if y_type == \"unknown\" or (\n",
            "                self._estimator_type == \"classifier\"\n",
            "                and y_type == \"multiclass-multioutput\"\n",
            "            ):\n",
            "                # FIXME: we could consider to support multiclass-multioutput if\n",
            "                # we introduce or reuse a constructor parameter (e.g.\n",
            "                # oob_score) allowing our user to pass a callable defining the\n",
            "                # scoring strategy on OOB sample.\n",
            "                raise ValueError(\n",
            "                    \"The type of target cannot be used to compute OOB \"\n",
            "                    f\"estimates. Got {y_type} while only the following are \"\n",
            "                    \"supported: continuous, continuous-multioutput, binary, \"\n",
            "                    \"multiclass, multilabel-indicator.\"\n",
            "                )\n",
            "\n",
            "            if callable(self.oob_score):\n",
            "                self._set_oob_score_and_attributes(\n",
            "                    X, y, scoring_function=self.oob_score\n",
            "                )\n",
            "            else:\n",
            "                self._set_oob_score_and_attributes(X, y)\n",
            "\n",
            "        # Decapsulate classes_ attributes\n",
            "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
            "            self.n_classes_ = self.n_classes_[0]\n",
            "            self.classes_ = self.classes_[0]\n",
            "\n",
            "        return self\n",
            "\n",
            "    @abstractmethod\n",
            "    def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n",
            "        \"\"\"Compute and set the OOB score and attributes.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : array-like of shape (n_samples, n_features)\n",
            "            The data matrix.\n",
            "        y : ndarray of shape (n_samples, n_outputs)\n",
            "            The target matrix.\n",
            "        scoring_function : callable, default=None\n",
            "            Scoring function for OOB score. Default depends on whether\n",
            "            this is a regression (R2 score) or classification problem\n",
            "            (accuracy score).\n",
            "        \"\"\"\n",
            "\n",
            "    def _compute_oob_predictions(self, X, y):\n",
            "        \"\"\"Compute and set the OOB score.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : array-like of shape (n_samples, n_features)\n",
            "            The data matrix.\n",
            "        y : ndarray of shape (n_samples, n_outputs)\n",
            "            The target matrix.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \\\n",
            "                (n_samples, 1, n_outputs)\n",
            "            The OOB predictions.\n",
            "        \"\"\"\n",
            "        # Prediction requires X to be in CSR format\n",
            "        if issparse(X):\n",
            "            X = X.tocsr()\n",
            "\n",
            "        n_samples = y.shape[0]\n",
            "        n_outputs = self.n_outputs_\n",
            "        if is_classifier(self) and hasattr(self, \"n_classes_\"):\n",
            "            # n_classes_ is a ndarray at this stage\n",
            "            # all the supported type of target will have the same number of\n",
            "            # classes in all outputs\n",
            "            oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)\n",
            "        else:\n",
            "            # for regression, n_classes_ does not exist and we create an empty\n",
            "            # axis to be consistent with the classification case and make\n",
            "            # the array operations compatible with the 2 settings\n",
            "            oob_pred_shape = (n_samples, 1, n_outputs)\n",
            "\n",
            "        oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)\n",
            "        n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)\n",
            "\n",
            "        n_samples_bootstrap = _get_n_samples_bootstrap(\n",
            "            n_samples,\n",
            "            self.max_samples,\n",
            "        )\n",
            "        for estimator in self.estimators_:\n",
            "            unsampled_indices = _generate_unsampled_indices(\n",
            "                estimator.random_state,\n",
            "                n_samples,\n",
            "                n_samples_bootstrap,\n",
            "            )\n",
            "\n",
            "            y_pred = self._get_oob_predictions(estimator, X[unsampled_indices, :])\n",
            "            oob_pred[unsampled_indices, ...] += y_pred\n",
            "            n_oob_pred[unsampled_indices, :] += 1\n",
            "\n",
            "        for k in range(n_outputs):\n",
            "            if (n_oob_pred == 0).any():\n",
            "                warn(\n",
            "                    (\n",
            "                        \"Some inputs do not have OOB scores. This probably means \"\n",
            "                        \"too few trees were used to compute any reliable OOB \"\n",
            "                        \"estimates.\"\n",
            "                    ),\n",
            "                    UserWarning,\n",
            "                )\n",
            "                n_oob_pred[n_oob_pred == 0] = 1\n",
            "            oob_pred[..., k] /= n_oob_pred[..., [k]]\n",
            "\n",
            "        return oob_pred\n",
            "\n",
            "    def _validate_y_class_weight(self, y):\n",
            "        # Default implementation\n",
            "        return y, None\n",
            "\n",
            "    def _validate_X_predict(self, X):\n",
            "        \"\"\"\n",
            "        Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n",
            "        check_is_fitted(self)\n",
            "        if self.estimators_[0]._support_missing_values(X):\n",
            "            force_all_finite = \"allow-nan\"\n",
            "        else:\n",
            "            force_all_finite = True\n",
            "\n",
            "        X = self._validate_data(\n",
            "            X,\n",
            "            dtype=DTYPE,\n",
            "            accept_sparse=\"csr\",\n",
            "            reset=False,\n",
            "            force_all_finite=force_all_finite,\n",
            "        )\n",
            "        if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n",
            "            raise ValueError(\"No support for np.int64 index based sparse matrices\")\n",
            "        return X\n",
            "\n",
            "    @property\n",
            "    def feature_importances_(self):\n",
            "        \"\"\"\n",
            "        The impurity-based feature importances.\n",
            "\n",
            "        The higher, the more important the feature.\n",
            "        The importance of a feature is computed as the (normalized)\n",
            "        total reduction of the criterion brought by that feature.  It is also\n",
            "        known as the Gini importance.\n",
            "\n",
            "        Warning: impurity-based feature importances can be misleading for\n",
            "        high cardinality features (many unique values). See\n",
            "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        feature_importances_ : ndarray of shape (n_features,)\n",
            "            The values of this array sum to 1, unless all trees are single node\n",
            "            trees consisting of only the root node, in which case it will be an\n",
            "            array of zeros.\n",
            "        \"\"\"\n",
            "        check_is_fitted(self)\n",
            "\n",
            "        all_importances = Parallel(n_jobs=self.n_jobs, prefer=\"threads\")(\n",
            "            delayed(getattr)(tree, \"feature_importances_\")\n",
            "            for tree in self.estimators_\n",
            "            if tree.tree_.node_count > 1\n",
            "        )\n",
            "\n",
            "        if not all_importances:\n",
            "            return np.zeros(self.n_features_in_, dtype=np.float64)\n",
            "\n",
            "        all_importances = np.mean(all_importances, axis=0, dtype=np.float64)\n",
            "        return all_importances / np.sum(all_importances)\n",
            "\n",
            "    def _get_estimators_indices(self):\n",
            "        # Get drawn indices along both sample and feature axes\n",
            "        for tree in self.estimators_:\n",
            "            if not self.bootstrap:\n",
            "                yield np.arange(self._n_samples, dtype=np.int32)\n",
            "            else:\n",
            "                # tree.random_state is actually an immutable integer seed rather\n",
            "                # than a mutable RandomState instance, so it's safe to use it\n",
            "                # repeatedly when calling this property.\n",
            "                seed = tree.random_state\n",
            "                # Operations accessing random_state must be performed identically\n",
            "                # to those in `_parallel_build_trees()`\n",
            "                yield _generate_sample_indices(\n",
            "                    seed, self._n_samples, self._n_samples_bootstrap\n",
            "                )\n",
            "\n",
            "    @property\n",
            "    def estimators_samples_(self):\n",
            "        \"\"\"The subset of drawn samples for each base estimator.\n",
            "\n",
            "        Returns a dynamically generated list of indices identifying\n",
            "        the samples used for fitting each member of the ensemble, i.e.,\n",
            "        the in-bag samples.\n",
            "\n",
            "        Note: the list is re-created at each call to the property in order\n",
            "        to reduce the object memory footprint by not storing the sampling\n",
            "        data. Thus fetching the property may be slower than expected.\n",
            "        \"\"\"\n",
            "        return [sample_indices for sample_indices in self._get_estimators_indices()]\n",
            "\n",
            "    def _more_tags(self):\n",
            "        # Only the criterion is required to determine if the tree supports\n",
            "        # missing values\n",
            "        estimator = type(self.estimator)(criterion=self.criterion)\n",
            "        return {\"allow_nan\": _safe_tags(estimator, key=\"allow_nan\")}\n",
            "\n",
            "\n",
            "def _accumulate_prediction(predict, X, out, lock):\n",
            "    \"\"\"\n",
            "    This is a utility function for joblib's Parallel.\n",
            "\n",
            "    It can't go locally in ForestClassifier or ForestRegressor, because joblib\n",
            "    complains that it cannot pickle it when placed there.\n",
            "    \"\"\"\n",
            "    prediction = predict(X, check_input=False)\n",
            "    with lock:\n",
            "        if len(out) == 1:\n",
            "            out[0] += prediction\n",
            "        else:\n",
            "            for i in range(len(out)):\n",
            "                out[i] += prediction[i]\n",
            "\n",
            "\n",
            "class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):\n",
            "    \"\"\"\n",
            "    Base class for forest of trees-based classifiers.\n",
            "\n",
            "    Warning: This class should not be used directly. Use derived classes\n",
            "    instead.\n",
            "    \"\"\"\n",
            "\n",
            "    @abstractmethod\n",
            "    def __init__(\n",
            "        self,\n",
            "        estimator,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        estimator_params=tuple(),\n",
            "        bootstrap=False,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        class_weight=None,\n",
            "        max_samples=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=estimator,\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=estimator_params,\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            class_weight=class_weight,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "    @staticmethod\n",
            "    def _get_oob_predictions(tree, X):\n",
            "        \"\"\"Compute the OOB predictions for an individual tree.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        tree : DecisionTreeClassifier object\n",
            "            A single decision tree classifier.\n",
            "        X : ndarray of shape (n_samples, n_features)\n",
            "            The OOB samples.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        y_pred : ndarray of shape (n_samples, n_classes, n_outputs)\n",
            "            The OOB associated predictions.\n",
            "        \"\"\"\n",
            "        y_pred = tree.predict_proba(X, check_input=False)\n",
            "        y_pred = np.asarray(y_pred)\n",
            "        if y_pred.ndim == 2:\n",
            "            # binary and multiclass\n",
            "            y_pred = y_pred[..., np.newaxis]\n",
            "        else:\n",
            "            # Roll the first `n_outputs` axis to the last axis. We will reshape\n",
            "            # from a shape of (n_outputs, n_samples, n_classes) to a shape of\n",
            "            # (n_samples, n_classes, n_outputs).\n",
            "            y_pred = np.rollaxis(y_pred, axis=0, start=3)\n",
            "        return y_pred\n",
            "\n",
            "    def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n",
            "        \"\"\"Compute and set the OOB score and attributes.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : array-like of shape (n_samples, n_features)\n",
            "            The data matrix.\n",
            "        y : ndarray of shape (n_samples, n_outputs)\n",
            "            The target matrix.\n",
            "        scoring_function : callable, default=None\n",
            "            Scoring function for OOB score. Defaults to `accuracy_score`.\n",
            "        \"\"\"\n",
            "        self.oob_decision_function_ = super()._compute_oob_predictions(X, y)\n",
            "        if self.oob_decision_function_.shape[-1] == 1:\n",
            "            # drop the n_outputs axis if there is a single output\n",
            "            self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)\n",
            "\n",
            "        if scoring_function is None:\n",
            "            scoring_function = accuracy_score\n",
            "\n",
            "        self.oob_score_ = scoring_function(\n",
            "            y, np.argmax(self.oob_decision_function_, axis=1)\n",
            "        )\n",
            "\n",
            "    def _validate_y_class_weight(self, y):\n",
            "        check_classification_targets(y)\n",
            "\n",
            "        y = np.copy(y)\n",
            "        expanded_class_weight = None\n",
            "\n",
            "        if self.class_weight is not None:\n",
            "            y_original = np.copy(y)\n",
            "\n",
            "        self.classes_ = []\n",
            "        self.n_classes_ = []\n",
            "\n",
            "        y_store_unique_indices = np.zeros(y.shape, dtype=int)\n",
            "        for k in range(self.n_outputs_):\n",
            "            classes_k, y_store_unique_indices[:, k] = np.unique(\n",
            "                y[:, k], return_inverse=True\n",
            "            )\n",
            "            self.classes_.append(classes_k)\n",
            "            self.n_classes_.append(classes_k.shape[0])\n",
            "        y = y_store_unique_indices\n",
            "\n",
            "        if self.class_weight is not None:\n",
            "            valid_presets = (\"balanced\", \"balanced_subsample\")\n",
            "            if isinstance(self.class_weight, str):\n",
            "                if self.class_weight not in valid_presets:\n",
            "                    raise ValueError(\n",
            "                        \"Valid presets for class_weight include \"\n",
            "                        '\"balanced\" and \"balanced_subsample\".'\n",
            "                        'Given \"%s\".' % self.class_weight\n",
            "                    )\n",
            "                if self.warm_start:\n",
            "                    warn(\n",
            "                        'class_weight presets \"balanced\" or '\n",
            "                        '\"balanced_subsample\" are '\n",
            "                        \"not recommended for warm_start if the fitted data \"\n",
            "                        \"differs from the full dataset. In order to use \"\n",
            "                        '\"balanced\" weights, use compute_class_weight '\n",
            "                        '(\"balanced\", classes, y). In place of y you can use '\n",
            "                        \"a large enough sample of the full training set \"\n",
            "                        \"target to properly estimate the class frequency \"\n",
            "                        \"distributions. Pass the resulting weights as the \"\n",
            "                        \"class_weight parameter.\"\n",
            "                    )\n",
            "\n",
            "            if self.class_weight != \"balanced_subsample\" or not self.bootstrap:\n",
            "                if self.class_weight == \"balanced_subsample\":\n",
            "                    class_weight = \"balanced\"\n",
            "                else:\n",
            "                    class_weight = self.class_weight\n",
            "                expanded_class_weight = compute_sample_weight(class_weight, y_original)\n",
            "\n",
            "        return y, expanded_class_weight\n",
            "\n",
            "    def predict(self, X):\n",
            "        \"\"\"\n",
            "        Predict class for X.\n",
            "\n",
            "        The predicted class of an input sample is a vote by the trees in\n",
            "        the forest, weighted by their probability estimates. That is,\n",
            "        the predicted class is the one with highest mean probability\n",
            "        estimate across the trees.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
            "            The predicted classes.\n",
            "        \"\"\"\n",
            "        proba = self.predict_proba(X)\n",
            "\n",
            "        if self.n_outputs_ == 1:\n",
            "            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n",
            "\n",
            "        else:\n",
            "            n_samples = proba[0].shape[0]\n",
            "            # all dtypes should be the same, so just take the first\n",
            "            class_type = self.classes_[0].dtype\n",
            "            predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n",
            "\n",
            "            for k in range(self.n_outputs_):\n",
            "                predictions[:, k] = self.classes_[k].take(\n",
            "                    np.argmax(proba[k], axis=1), axis=0\n",
            "                )\n",
            "\n",
            "            return predictions\n",
            "\n",
            "    def predict_proba(self, X):\n",
            "        \"\"\"\n",
            "        Predict class probabilities for X.\n",
            "\n",
            "        The predicted class probabilities of an input sample are computed as\n",
            "        the mean predicted class probabilities of the trees in the forest.\n",
            "        The class probability of a single tree is the fraction of samples of\n",
            "        the same class in a leaf.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
            "            The class probabilities of the input samples. The order of the\n",
            "            classes corresponds to that in the attribute :term:`classes_`.\n",
            "        \"\"\"\n",
            "        check_is_fitted(self)\n",
            "        # Check data\n",
            "        X = self._validate_X_predict(X)\n",
            "\n",
            "        # Assign chunk of trees to jobs\n",
            "        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n",
            "\n",
            "        # avoid storing the output of every estimator by summing them here\n",
            "        all_proba = [\n",
            "            np.zeros((X.shape[0], j), dtype=np.float64)\n",
            "            for j in np.atleast_1d(self.n_classes_)\n",
            "        ]\n",
            "        lock = threading.Lock()\n",
            "        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
            "            delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)\n",
            "            for e in self.estimators_\n",
            "        )\n",
            "\n",
            "        for proba in all_proba:\n",
            "            proba /= len(self.estimators_)\n",
            "\n",
            "        if len(all_proba) == 1:\n",
            "            return all_proba[0]\n",
            "        else:\n",
            "            return all_proba\n",
            "\n",
            "    def predict_log_proba(self, X):\n",
            "        \"\"\"\n",
            "        Predict class log-probabilities for X.\n",
            "\n",
            "        The predicted class log-probabilities of an input sample is computed as\n",
            "        the log of the mean predicted class probabilities of the trees in the\n",
            "        forest.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
            "            The class probabilities of the input samples. The order of the\n",
            "            classes corresponds to that in the attribute :term:`classes_`.\n",
            "        \"\"\"\n",
            "        proba = self.predict_proba(X)\n",
            "\n",
            "        if self.n_outputs_ == 1:\n",
            "            return np.log(proba)\n",
            "\n",
            "        else:\n",
            "            for k in range(self.n_outputs_):\n",
            "                proba[k] = np.log(proba[k])\n",
            "\n",
            "            return proba\n",
            "\n",
            "    def _more_tags(self):\n",
            "        return {\"multilabel\": True}\n",
            "\n",
            "\n",
            "class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):\n",
            "    \"\"\"\n",
            "    Base class for forest of trees-based regressors.\n",
            "\n",
            "    Warning: This class should not be used directly. Use derived classes\n",
            "    instead.\n",
            "    \"\"\"\n",
            "\n",
            "    @abstractmethod\n",
            "    def __init__(\n",
            "        self,\n",
            "        estimator,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        estimator_params=tuple(),\n",
            "        bootstrap=False,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        max_samples=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator,\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=estimator_params,\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "    def predict(self, X):\n",
            "        \"\"\"\n",
            "        Predict regression target for X.\n",
            "\n",
            "        The predicted regression target of an input sample is computed as the\n",
            "        mean predicted regression targets of the trees in the forest.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Internally, its dtype will be converted to\n",
            "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            "            converted into a sparse ``csr_matrix``.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
            "            The predicted values.\n",
            "        \"\"\"\n",
            "        check_is_fitted(self)\n",
            "        # Check data\n",
            "        X = self._validate_X_predict(X)\n",
            "\n",
            "        # Assign chunk of trees to jobs\n",
            "        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n",
            "\n",
            "        # avoid storing the output of every estimator by summing them here\n",
            "        if self.n_outputs_ > 1:\n",
            "            y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)\n",
            "        else:\n",
            "            y_hat = np.zeros((X.shape[0]), dtype=np.float64)\n",
            "\n",
            "        # Parallel loop\n",
            "        lock = threading.Lock()\n",
            "        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
            "            delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)\n",
            "            for e in self.estimators_\n",
            "        )\n",
            "\n",
            "        y_hat /= len(self.estimators_)\n",
            "\n",
            "        return y_hat\n",
            "\n",
            "    @staticmethod\n",
            "    def _get_oob_predictions(tree, X):\n",
            "        \"\"\"Compute the OOB predictions for an individual tree.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        tree : DecisionTreeRegressor object\n",
            "            A single decision tree regressor.\n",
            "        X : ndarray of shape (n_samples, n_features)\n",
            "            The OOB samples.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        y_pred : ndarray of shape (n_samples, 1, n_outputs)\n",
            "            The OOB associated predictions.\n",
            "        \"\"\"\n",
            "        y_pred = tree.predict(X, check_input=False)\n",
            "        if y_pred.ndim == 1:\n",
            "            # single output regression\n",
            "            y_pred = y_pred[:, np.newaxis, np.newaxis]\n",
            "        else:\n",
            "            # multioutput regression\n",
            "            y_pred = y_pred[:, np.newaxis, :]\n",
            "        return y_pred\n",
            "\n",
            "    def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n",
            "        \"\"\"Compute and set the OOB score and attributes.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : array-like of shape (n_samples, n_features)\n",
            "            The data matrix.\n",
            "        y : ndarray of shape (n_samples, n_outputs)\n",
            "            The target matrix.\n",
            "        scoring_function : callable, default=None\n",
            "            Scoring function for OOB score. Defaults to `r2_score`.\n",
            "        \"\"\"\n",
            "        self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)\n",
            "        if self.oob_prediction_.shape[-1] == 1:\n",
            "            # drop the n_outputs axis if there is a single output\n",
            "            self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)\n",
            "\n",
            "        if scoring_function is None:\n",
            "            scoring_function = r2_score\n",
            "\n",
            "        self.oob_score_ = scoring_function(y, self.oob_prediction_)\n",
            "\n",
            "    def _compute_partial_dependence_recursion(self, grid, target_features):\n",
            "        \"\"\"Fast partial dependence computation.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        grid : ndarray of shape (n_samples, n_target_features), dtype=DTYPE\n",
            "            The grid points on which the partial dependence should be\n",
            "            evaluated.\n",
            "        target_features : ndarray of shape (n_target_features), dtype=np.intp\n",
            "            The set of target features for which the partial dependence\n",
            "            should be evaluated.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        averaged_predictions : ndarray of shape (n_samples,)\n",
            "            The value of the partial dependence function on each grid point.\n",
            "        \"\"\"\n",
            "        grid = np.asarray(grid, dtype=DTYPE, order=\"C\")\n",
            "        target_features = np.asarray(target_features, dtype=np.intp, order=\"C\")\n",
            "        averaged_predictions = np.zeros(\n",
            "            shape=grid.shape[0], dtype=np.float64, order=\"C\"\n",
            "        )\n",
            "\n",
            "        for tree in self.estimators_:\n",
            "            # Note: we don't sum in parallel because the GIL isn't released in\n",
            "            # the fast method.\n",
            "            tree.tree_.compute_partial_dependence(\n",
            "                grid, target_features, averaged_predictions\n",
            "            )\n",
            "        # Average over the forest\n",
            "        averaged_predictions /= len(self.estimators_)\n",
            "\n",
            "        return averaged_predictions\n",
            "\n",
            "    def _more_tags(self):\n",
            "        return {\"multilabel\": True}\n",
            "\n",
            "\n",
            "class RandomForestClassifier(ForestClassifier):\n",
            "    \"\"\"\n",
            "    A random forest classifier.\n",
            "\n",
            "    A random forest is a meta estimator that fits a number of decision tree\n",
            "    classifiers on various sub-samples of the dataset and uses averaging to\n",
            "    improve the predictive accuracy and control over-fitting.\n",
            "    Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
            "    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
            "    The sub-sample size is controlled with the `max_samples` parameter if\n",
            "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
            "    each tree.\n",
            "\n",
            "    For a comparison between tree-based ensemble models see the example\n",
            "    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
            "\n",
            "    Read more in the :ref:`User Guide <forest>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_estimators : int, default=100\n",
            "        The number of trees in the forest.\n",
            "\n",
            "        .. versionchanged:: 0.22\n",
            "           The default value of ``n_estimators`` changed from 10 to 100\n",
            "           in 0.22.\n",
            "\n",
            "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
            "        The function to measure the quality of a split. Supported criteria are\n",
            "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
            "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
            "        Note: This parameter is tree-specific.\n",
            "\n",
            "    max_depth : int, default=None\n",
            "        The maximum depth of the tree. If None, then nodes are expanded until\n",
            "        all leaves are pure or until all leaves contain less than\n",
            "        min_samples_split samples.\n",
            "\n",
            "    min_samples_split : int or float, default=2\n",
            "        The minimum number of samples required to split an internal node:\n",
            "\n",
            "        - If int, then consider `min_samples_split` as the minimum number.\n",
            "        - If float, then `min_samples_split` is a fraction and\n",
            "          `ceil(min_samples_split * n_samples)` are the minimum\n",
            "          number of samples for each split.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_samples_leaf : int or float, default=1\n",
            "        The minimum number of samples required to be at a leaf node.\n",
            "        A split point at any depth will only be considered if it leaves at\n",
            "        least ``min_samples_leaf`` training samples in each of the left and\n",
            "        right branches.  This may have the effect of smoothing the model,\n",
            "        especially in regression.\n",
            "\n",
            "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
            "        - If float, then `min_samples_leaf` is a fraction and\n",
            "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            "          number of samples for each node.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_weight_fraction_leaf : float, default=0.0\n",
            "        The minimum weighted fraction of the sum total of weights (of all\n",
            "        the input samples) required to be at a leaf node. Samples have\n",
            "        equal weight when sample_weight is not provided.\n",
            "\n",
            "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
            "        The number of features to consider when looking for the best split:\n",
            "\n",
            "        - If int, then consider `max_features` features at each split.\n",
            "        - If float, then `max_features` is a fraction and\n",
            "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
            "          split.\n",
            "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            "        - If \"log2\", then `max_features=log2(n_features)`.\n",
            "        - If None, then `max_features=n_features`.\n",
            "\n",
            "        .. versionchanged:: 1.1\n",
            "            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
            "\n",
            "        Note: the search for a split does not stop until at least one\n",
            "        valid partition of the node samples is found, even if it requires to\n",
            "        effectively inspect more than ``max_features`` features.\n",
            "\n",
            "    max_leaf_nodes : int, default=None\n",
            "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            "        Best nodes are defined as relative reduction in impurity.\n",
            "        If None then unlimited number of leaf nodes.\n",
            "\n",
            "    min_impurity_decrease : float, default=0.0\n",
            "        A node will be split if this split induces a decrease of the impurity\n",
            "        greater than or equal to this value.\n",
            "\n",
            "        The weighted impurity decrease equation is the following::\n",
            "\n",
            "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            "                                - N_t_L / N_t * left_impurity)\n",
            "\n",
            "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
            "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
            "\n",
            "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            "        if ``sample_weight`` is passed.\n",
            "\n",
            "        .. versionadded:: 0.19\n",
            "\n",
            "    bootstrap : bool, default=True\n",
            "        Whether bootstrap samples are used when building trees. If False, the\n",
            "        whole dataset is used to build each tree.\n",
            "\n",
            "    oob_score : bool or callable, default=False\n",
            "        Whether to use out-of-bag samples to estimate the generalization score.\n",
            "        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
            "        Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
            "        custom metric. Only available if `bootstrap=True`.\n",
            "\n",
            "    n_jobs : int, default=None\n",
            "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            "        context. ``-1`` means using all processors. See :term:`Glossary\n",
            "        <n_jobs>` for more details.\n",
            "\n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Controls both the randomness of the bootstrapping of the samples used\n",
            "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
            "        features to consider when looking for the best split at each node\n",
            "        (if ``max_features < n_features``).\n",
            "        See :term:`Glossary <random_state>` for details.\n",
            "\n",
            "    verbose : int, default=0\n",
            "        Controls the verbosity when fitting and predicting.\n",
            "\n",
            "    warm_start : bool, default=False\n",
            "        When set to ``True``, reuse the solution of the previous call to fit\n",
            "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
            "        new forest. See :term:`Glossary <warm_start>` and\n",
            "        :ref:`tree_ensemble_warm_start` for details.\n",
            "\n",
            "    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n",
            "            default=None\n",
            "        Weights associated with classes in the form ``{class_label: weight}``.\n",
            "        If not given, all classes are supposed to have weight one. For\n",
            "        multi-output problems, a list of dicts can be provided in the same\n",
            "        order as the columns of y.\n",
            "\n",
            "        Note that for multioutput (including multilabel) weights should be\n",
            "        defined for each class of every column in its own dict. For example,\n",
            "        for four-class multilabel classification weights should be\n",
            "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            "\n",
            "        The \"balanced\" mode uses the values of y to automatically adjust\n",
            "        weights inversely proportional to class frequencies in the input data\n",
            "        as ``n_samples / (n_classes * np.bincount(y))``\n",
            "\n",
            "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
            "        weights are computed based on the bootstrap sample for every tree\n",
            "        grown.\n",
            "\n",
            "        For multi-output, the weights of each column of y will be multiplied.\n",
            "\n",
            "        Note that these weights will be multiplied with sample_weight (passed\n",
            "        through the fit method) if sample_weight is specified.\n",
            "\n",
            "    ccp_alpha : non-negative float, default=0.0\n",
            "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            "        subtree with the largest cost complexity that is smaller than\n",
            "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            "        :ref:`minimal_cost_complexity_pruning` for details.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    max_samples : int or float, default=None\n",
            "        If bootstrap is True, the number of samples to draw from X\n",
            "        to train each base estimator.\n",
            "\n",
            "        - If None (default), then draw `X.shape[0]` samples.\n",
            "        - If int, then draw `max_samples` samples.\n",
            "        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
            "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
            "        Indicates the monotonicity constraint to enforce on each feature.\n",
            "          - 1: monotonic increase\n",
            "          - 0: no constraint\n",
            "          - -1: monotonic decrease\n",
            "\n",
            "        If monotonic_cst is None, no constraints are applied.\n",
            "\n",
            "        Monotonicity constraints are not supported for:\n",
            "          - multiclass classifications (i.e. when `n_classes > 2`),\n",
            "          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
            "          - classifications trained on data with missing values.\n",
            "\n",
            "        The constraints hold over the probability of the positive class.\n",
            "\n",
            "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
            "        The child estimator template used to create the collection of fitted\n",
            "        sub-estimators.\n",
            "\n",
            "        .. versionadded:: 1.2\n",
            "           `base_estimator_` was renamed to `estimator_`.\n",
            "\n",
            "    estimators_ : list of DecisionTreeClassifier\n",
            "        The collection of fitted sub-estimators.\n",
            "\n",
            "    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
            "        The classes labels (single output problem), or a list of arrays of\n",
            "        class labels (multi-output problem).\n",
            "\n",
            "    n_classes_ : int or list\n",
            "        The number of classes (single output problem), or a list containing the\n",
            "        number of classes for each output (multi-output problem).\n",
            "\n",
            "    n_features_in_ : int\n",
            "        Number of features seen during :term:`fit`.\n",
            "\n",
            "        .. versionadded:: 0.24\n",
            "\n",
            "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            "        Names of features seen during :term:`fit`. Defined only when `X`\n",
            "        has feature names that are all strings.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "\n",
            "    n_outputs_ : int\n",
            "        The number of outputs when ``fit`` is performed.\n",
            "\n",
            "    feature_importances_ : ndarray of shape (n_features,)\n",
            "        The impurity-based feature importances.\n",
            "        The higher, the more important the feature.\n",
            "        The importance of a feature is computed as the (normalized)\n",
            "        total reduction of the criterion brought by that feature.  It is also\n",
            "        known as the Gini importance.\n",
            "\n",
            "        Warning: impurity-based feature importances can be misleading for\n",
            "        high cardinality features (many unique values). See\n",
            "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            "\n",
            "    oob_score_ : float\n",
            "        Score of the training dataset obtained using an out-of-bag estimate.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\n",
            "            (n_samples, n_classes, n_outputs)\n",
            "        Decision function computed with out-of-bag estimate on the training\n",
            "        set. If n_estimators is small it might be possible that a data point\n",
            "        was never left out during the bootstrap. In this case,\n",
            "        `oob_decision_function_` might contain NaN. This attribute exists\n",
            "        only when ``oob_score`` is True.\n",
            "\n",
            "    estimators_samples_ : list of arrays\n",
            "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
            "        estimator. Each subset is defined by an array of the indices selected.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
            "    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
            "        tree classifiers.\n",
            "    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
            "        Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
            "        10_000).\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The default values for the parameters controlling the size of the trees\n",
            "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            "    unpruned trees which can potentially be very large on some data sets. To\n",
            "    reduce memory consumption, the complexity and size of the trees should be\n",
            "    controlled by setting those parameter values.\n",
            "\n",
            "    The features are always randomly permuted at each split. Therefore,\n",
            "    the best found split may vary, even with the same training data,\n",
            "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            "    of the criterion is identical for several splits enumerated during the\n",
            "    search of the best split. To obtain a deterministic behaviour during\n",
            "    fitting, ``random_state`` has to be fixed.\n",
            "\n",
            "    References\n",
            "    ----------\n",
            "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.ensemble import RandomForestClassifier\n",
            "    >>> from sklearn.datasets import make_classification\n",
            "    >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
            "    ...                            n_informative=2, n_redundant=0,\n",
            "    ...                            random_state=0, shuffle=False)\n",
            "    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
            "    >>> clf.fit(X, y)\n",
            "    RandomForestClassifier(...)\n",
            "    >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            "    [1]\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        **ForestClassifier._parameter_constraints,\n",
            "        **DecisionTreeClassifier._parameter_constraints,\n",
            "        \"class_weight\": [\n",
            "            StrOptions({\"balanced_subsample\", \"balanced\"}),\n",
            "            dict,\n",
            "            list,\n",
            "            None,\n",
            "        ],\n",
            "    }\n",
            "    _parameter_constraints.pop(\"splitter\")\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        criterion=\"gini\",\n",
            "        max_depth=None,\n",
            "        min_samples_split=2,\n",
            "        min_samples_leaf=1,\n",
            "        min_weight_fraction_leaf=0.0,\n",
            "        max_features=\"sqrt\",\n",
            "        max_leaf_nodes=None,\n",
            "        min_impurity_decrease=0.0,\n",
            "        bootstrap=True,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        class_weight=None,\n",
            "        ccp_alpha=0.0,\n",
            "        max_samples=None,\n",
            "        monotonic_cst=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=DecisionTreeClassifier(),\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=(\n",
            "                \"criterion\",\n",
            "                \"max_depth\",\n",
            "                \"min_samples_split\",\n",
            "                \"min_samples_leaf\",\n",
            "                \"min_weight_fraction_leaf\",\n",
            "                \"max_features\",\n",
            "                \"max_leaf_nodes\",\n",
            "                \"min_impurity_decrease\",\n",
            "                \"random_state\",\n",
            "                \"ccp_alpha\",\n",
            "                \"monotonic_cst\",\n",
            "            ),\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            class_weight=class_weight,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "        self.criterion = criterion\n",
            "        self.max_depth = max_depth\n",
            "        self.min_samples_split = min_samples_split\n",
            "        self.min_samples_leaf = min_samples_leaf\n",
            "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
            "        self.max_features = max_features\n",
            "        self.max_leaf_nodes = max_leaf_nodes\n",
            "        self.min_impurity_decrease = min_impurity_decrease\n",
            "        self.monotonic_cst = monotonic_cst\n",
            "        self.ccp_alpha = ccp_alpha\n",
            "\n",
            "\n",
            "class RandomForestRegressor(ForestRegressor):\n",
            "    \"\"\"\n",
            "    A random forest regressor.\n",
            "\n",
            "    A random forest is a meta estimator that fits a number of decision tree\n",
            "    regressors on various sub-samples of the dataset and uses averaging to\n",
            "    improve the predictive accuracy and control over-fitting.\n",
            "    Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
            "    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
            "    The sub-sample size is controlled with the `max_samples` parameter if\n",
            "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
            "    each tree.\n",
            "\n",
            "    For a comparison between tree-based ensemble models see the example\n",
            "    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
            "\n",
            "    Read more in the :ref:`User Guide <forest>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_estimators : int, default=100\n",
            "        The number of trees in the forest.\n",
            "\n",
            "        .. versionchanged:: 0.22\n",
            "           The default value of ``n_estimators`` changed from 10 to 100\n",
            "           in 0.22.\n",
            "\n",
            "    criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\n",
            "            default=\"squared_error\"\n",
            "        The function to measure the quality of a split. Supported criteria\n",
            "        are \"squared_error\" for the mean squared error, which is equal to\n",
            "        variance reduction as feature selection criterion and minimizes the L2\n",
            "        loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
            "        mean squared error with Friedman's improvement score for potential\n",
            "        splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
            "        the L1 loss using the median of each terminal node, and \"poisson\" which\n",
            "        uses reduction in Poisson deviance to find splits.\n",
            "        Training using \"absolute_error\" is significantly slower\n",
            "        than when using \"squared_error\".\n",
            "\n",
            "        .. versionadded:: 0.18\n",
            "           Mean Absolute Error (MAE) criterion.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "           Poisson criterion.\n",
            "\n",
            "    max_depth : int, default=None\n",
            "        The maximum depth of the tree. If None, then nodes are expanded until\n",
            "        all leaves are pure or until all leaves contain less than\n",
            "        min_samples_split samples.\n",
            "\n",
            "    min_samples_split : int or float, default=2\n",
            "        The minimum number of samples required to split an internal node:\n",
            "\n",
            "        - If int, then consider `min_samples_split` as the minimum number.\n",
            "        - If float, then `min_samples_split` is a fraction and\n",
            "          `ceil(min_samples_split * n_samples)` are the minimum\n",
            "          number of samples for each split.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_samples_leaf : int or float, default=1\n",
            "        The minimum number of samples required to be at a leaf node.\n",
            "        A split point at any depth will only be considered if it leaves at\n",
            "        least ``min_samples_leaf`` training samples in each of the left and\n",
            "        right branches.  This may have the effect of smoothing the model,\n",
            "        especially in regression.\n",
            "\n",
            "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
            "        - If float, then `min_samples_leaf` is a fraction and\n",
            "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            "          number of samples for each node.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_weight_fraction_leaf : float, default=0.0\n",
            "        The minimum weighted fraction of the sum total of weights (of all\n",
            "        the input samples) required to be at a leaf node. Samples have\n",
            "        equal weight when sample_weight is not provided.\n",
            "\n",
            "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
            "        The number of features to consider when looking for the best split:\n",
            "\n",
            "        - If int, then consider `max_features` features at each split.\n",
            "        - If float, then `max_features` is a fraction and\n",
            "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
            "          split.\n",
            "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            "        - If \"log2\", then `max_features=log2(n_features)`.\n",
            "        - If None or 1.0, then `max_features=n_features`.\n",
            "\n",
            "        .. note::\n",
            "            The default of 1.0 is equivalent to bagged trees and more\n",
            "            randomness can be achieved by setting smaller values, e.g. 0.3.\n",
            "\n",
            "        .. versionchanged:: 1.1\n",
            "            The default of `max_features` changed from `\"auto\"` to 1.0.\n",
            "\n",
            "        Note: the search for a split does not stop until at least one\n",
            "        valid partition of the node samples is found, even if it requires to\n",
            "        effectively inspect more than ``max_features`` features.\n",
            "\n",
            "    max_leaf_nodes : int, default=None\n",
            "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            "        Best nodes are defined as relative reduction in impurity.\n",
            "        If None then unlimited number of leaf nodes.\n",
            "\n",
            "    min_impurity_decrease : float, default=0.0\n",
            "        A node will be split if this split induces a decrease of the impurity\n",
            "        greater than or equal to this value.\n",
            "\n",
            "        The weighted impurity decrease equation is the following::\n",
            "\n",
            "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            "                                - N_t_L / N_t * left_impurity)\n",
            "\n",
            "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
            "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
            "\n",
            "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            "        if ``sample_weight`` is passed.\n",
            "\n",
            "        .. versionadded:: 0.19\n",
            "\n",
            "    bootstrap : bool, default=True\n",
            "        Whether bootstrap samples are used when building trees. If False, the\n",
            "        whole dataset is used to build each tree.\n",
            "\n",
            "    oob_score : bool or callable, default=False\n",
            "        Whether to use out-of-bag samples to estimate the generalization score.\n",
            "        By default, :func:`~sklearn.metrics.r2_score` is used.\n",
            "        Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
            "        custom metric. Only available if `bootstrap=True`.\n",
            "\n",
            "    n_jobs : int, default=None\n",
            "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            "        context. ``-1`` means using all processors. See :term:`Glossary\n",
            "        <n_jobs>` for more details.\n",
            "\n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Controls both the randomness of the bootstrapping of the samples used\n",
            "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
            "        features to consider when looking for the best split at each node\n",
            "        (if ``max_features < n_features``).\n",
            "        See :term:`Glossary <random_state>` for details.\n",
            "\n",
            "    verbose : int, default=0\n",
            "        Controls the verbosity when fitting and predicting.\n",
            "\n",
            "    warm_start : bool, default=False\n",
            "        When set to ``True``, reuse the solution of the previous call to fit\n",
            "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
            "        new forest. See :term:`Glossary <warm_start>` and\n",
            "        :ref:`tree_ensemble_warm_start` for details.\n",
            "\n",
            "    ccp_alpha : non-negative float, default=0.0\n",
            "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            "        subtree with the largest cost complexity that is smaller than\n",
            "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            "        :ref:`minimal_cost_complexity_pruning` for details.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    max_samples : int or float, default=None\n",
            "        If bootstrap is True, the number of samples to draw from X\n",
            "        to train each base estimator.\n",
            "\n",
            "        - If None (default), then draw `X.shape[0]` samples.\n",
            "        - If int, then draw `max_samples` samples.\n",
            "        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
            "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
            "        Indicates the monotonicity constraint to enforce on each feature.\n",
            "          - 1: monotonically increasing\n",
            "          - 0: no constraint\n",
            "          - -1: monotonically decreasing\n",
            "\n",
            "        If monotonic_cst is None, no constraints are applied.\n",
            "\n",
            "        Monotonicity constraints are not supported for:\n",
            "          - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
            "          - regressions trained on data with missing values.\n",
            "\n",
            "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
            "        The child estimator template used to create the collection of fitted\n",
            "        sub-estimators.\n",
            "\n",
            "        .. versionadded:: 1.2\n",
            "           `base_estimator_` was renamed to `estimator_`.\n",
            "\n",
            "    estimators_ : list of DecisionTreeRegressor\n",
            "        The collection of fitted sub-estimators.\n",
            "\n",
            "    feature_importances_ : ndarray of shape (n_features,)\n",
            "        The impurity-based feature importances.\n",
            "        The higher, the more important the feature.\n",
            "        The importance of a feature is computed as the (normalized)\n",
            "        total reduction of the criterion brought by that feature.  It is also\n",
            "        known as the Gini importance.\n",
            "\n",
            "        Warning: impurity-based feature importances can be misleading for\n",
            "        high cardinality features (many unique values). See\n",
            "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            "\n",
            "    n_features_in_ : int\n",
            "        Number of features seen during :term:`fit`.\n",
            "\n",
            "        .. versionadded:: 0.24\n",
            "\n",
            "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            "        Names of features seen during :term:`fit`. Defined only when `X`\n",
            "        has feature names that are all strings.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "\n",
            "    n_outputs_ : int\n",
            "        The number of outputs when ``fit`` is performed.\n",
            "\n",
            "    oob_score_ : float\n",
            "        Score of the training dataset obtained using an out-of-bag estimate.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
            "        Prediction computed with out-of-bag estimate on the training set.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    estimators_samples_ : list of arrays\n",
            "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
            "        estimator. Each subset is defined by an array of the indices selected.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
            "    sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
            "        tree regressors.\n",
            "    sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\n",
            "        Boosting Regression Tree, very fast for big datasets (n_samples >=\n",
            "        10_000).\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The default values for the parameters controlling the size of the trees\n",
            "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            "    unpruned trees which can potentially be very large on some data sets. To\n",
            "    reduce memory consumption, the complexity and size of the trees should be\n",
            "    controlled by setting those parameter values.\n",
            "\n",
            "    The features are always randomly permuted at each split. Therefore,\n",
            "    the best found split may vary, even with the same training data,\n",
            "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            "    of the criterion is identical for several splits enumerated during the\n",
            "    search of the best split. To obtain a deterministic behaviour during\n",
            "    fitting, ``random_state`` has to be fixed.\n",
            "\n",
            "    The default value ``max_features=1.0`` uses ``n_features``\n",
            "    rather than ``n_features / 3``. The latter was originally suggested in\n",
            "    [1], whereas the former was more recently justified empirically in [2].\n",
            "\n",
            "    References\n",
            "    ----------\n",
            "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            "\n",
            "    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
            "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.ensemble import RandomForestRegressor\n",
            "    >>> from sklearn.datasets import make_regression\n",
            "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
            "    ...                        random_state=0, shuffle=False)\n",
            "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
            "    >>> regr.fit(X, y)\n",
            "    RandomForestRegressor(...)\n",
            "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
            "    [-8.32987858]\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        **ForestRegressor._parameter_constraints,\n",
            "        **DecisionTreeRegressor._parameter_constraints,\n",
            "    }\n",
            "    _parameter_constraints.pop(\"splitter\")\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        criterion=\"squared_error\",\n",
            "        max_depth=None,\n",
            "        min_samples_split=2,\n",
            "        min_samples_leaf=1,\n",
            "        min_weight_fraction_leaf=0.0,\n",
            "        max_features=1.0,\n",
            "        max_leaf_nodes=None,\n",
            "        min_impurity_decrease=0.0,\n",
            "        bootstrap=True,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        ccp_alpha=0.0,\n",
            "        max_samples=None,\n",
            "        monotonic_cst=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=DecisionTreeRegressor(),\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=(\n",
            "                \"criterion\",\n",
            "                \"max_depth\",\n",
            "                \"min_samples_split\",\n",
            "                \"min_samples_leaf\",\n",
            "                \"min_weight_fraction_leaf\",\n",
            "                \"max_features\",\n",
            "                \"max_leaf_nodes\",\n",
            "                \"min_impurity_decrease\",\n",
            "                \"random_state\",\n",
            "                \"ccp_alpha\",\n",
            "                \"monotonic_cst\",\n",
            "            ),\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "        self.criterion = criterion\n",
            "        self.max_depth = max_depth\n",
            "        self.min_samples_split = min_samples_split\n",
            "        self.min_samples_leaf = min_samples_leaf\n",
            "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
            "        self.max_features = max_features\n",
            "        self.max_leaf_nodes = max_leaf_nodes\n",
            "        self.min_impurity_decrease = min_impurity_decrease\n",
            "        self.ccp_alpha = ccp_alpha\n",
            "        self.monotonic_cst = monotonic_cst\n",
            "\n",
            "\n",
            "class ExtraTreesClassifier(ForestClassifier):\n",
            "    \"\"\"\n",
            "    An extra-trees classifier.\n",
            "\n",
            "    This class implements a meta estimator that fits a number of\n",
            "    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
            "    of the dataset and uses averaging to improve the predictive accuracy\n",
            "    and control over-fitting.\n",
            "\n",
            "    Read more in the :ref:`User Guide <forest>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_estimators : int, default=100\n",
            "        The number of trees in the forest.\n",
            "\n",
            "        .. versionchanged:: 0.22\n",
            "           The default value of ``n_estimators`` changed from 10 to 100\n",
            "           in 0.22.\n",
            "\n",
            "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
            "        The function to measure the quality of a split. Supported criteria are\n",
            "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
            "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
            "        Note: This parameter is tree-specific.\n",
            "\n",
            "    max_depth : int, default=None\n",
            "        The maximum depth of the tree. If None, then nodes are expanded until\n",
            "        all leaves are pure or until all leaves contain less than\n",
            "        min_samples_split samples.\n",
            "\n",
            "    min_samples_split : int or float, default=2\n",
            "        The minimum number of samples required to split an internal node:\n",
            "\n",
            "        - If int, then consider `min_samples_split` as the minimum number.\n",
            "        - If float, then `min_samples_split` is a fraction and\n",
            "          `ceil(min_samples_split * n_samples)` are the minimum\n",
            "          number of samples for each split.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_samples_leaf : int or float, default=1\n",
            "        The minimum number of samples required to be at a leaf node.\n",
            "        A split point at any depth will only be considered if it leaves at\n",
            "        least ``min_samples_leaf`` training samples in each of the left and\n",
            "        right branches.  This may have the effect of smoothing the model,\n",
            "        especially in regression.\n",
            "\n",
            "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
            "        - If float, then `min_samples_leaf` is a fraction and\n",
            "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            "          number of samples for each node.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_weight_fraction_leaf : float, default=0.0\n",
            "        The minimum weighted fraction of the sum total of weights (of all\n",
            "        the input samples) required to be at a leaf node. Samples have\n",
            "        equal weight when sample_weight is not provided.\n",
            "\n",
            "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
            "        The number of features to consider when looking for the best split:\n",
            "\n",
            "        - If int, then consider `max_features` features at each split.\n",
            "        - If float, then `max_features` is a fraction and\n",
            "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
            "          split.\n",
            "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            "        - If \"log2\", then `max_features=log2(n_features)`.\n",
            "        - If None, then `max_features=n_features`.\n",
            "\n",
            "        .. versionchanged:: 1.1\n",
            "            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
            "\n",
            "        Note: the search for a split does not stop until at least one\n",
            "        valid partition of the node samples is found, even if it requires to\n",
            "        effectively inspect more than ``max_features`` features.\n",
            "\n",
            "    max_leaf_nodes : int, default=None\n",
            "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            "        Best nodes are defined as relative reduction in impurity.\n",
            "        If None then unlimited number of leaf nodes.\n",
            "\n",
            "    min_impurity_decrease : float, default=0.0\n",
            "        A node will be split if this split induces a decrease of the impurity\n",
            "        greater than or equal to this value.\n",
            "\n",
            "        The weighted impurity decrease equation is the following::\n",
            "\n",
            "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            "                                - N_t_L / N_t * left_impurity)\n",
            "\n",
            "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
            "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
            "\n",
            "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            "        if ``sample_weight`` is passed.\n",
            "\n",
            "        .. versionadded:: 0.19\n",
            "\n",
            "    bootstrap : bool, default=False\n",
            "        Whether bootstrap samples are used when building trees. If False, the\n",
            "        whole dataset is used to build each tree.\n",
            "\n",
            "    oob_score : bool or callable, default=False\n",
            "        Whether to use out-of-bag samples to estimate the generalization score.\n",
            "        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
            "        Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
            "        custom metric. Only available if `bootstrap=True`.\n",
            "\n",
            "    n_jobs : int, default=None\n",
            "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            "        context. ``-1`` means using all processors. See :term:`Glossary\n",
            "        <n_jobs>` for more details.\n",
            "\n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Controls 3 sources of randomness:\n",
            "\n",
            "        - the bootstrapping of the samples used when building trees\n",
            "          (if ``bootstrap=True``)\n",
            "        - the sampling of the features to consider when looking for the best\n",
            "          split at each node (if ``max_features < n_features``)\n",
            "        - the draw of the splits for each of the `max_features`\n",
            "\n",
            "        See :term:`Glossary <random_state>` for details.\n",
            "\n",
            "    verbose : int, default=0\n",
            "        Controls the verbosity when fitting and predicting.\n",
            "\n",
            "    warm_start : bool, default=False\n",
            "        When set to ``True``, reuse the solution of the previous call to fit\n",
            "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
            "        new forest. See :term:`Glossary <warm_start>` and\n",
            "        :ref:`tree_ensemble_warm_start` for details.\n",
            "\n",
            "    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n",
            "            default=None\n",
            "        Weights associated with classes in the form ``{class_label: weight}``.\n",
            "        If not given, all classes are supposed to have weight one. For\n",
            "        multi-output problems, a list of dicts can be provided in the same\n",
            "        order as the columns of y.\n",
            "\n",
            "        Note that for multioutput (including multilabel) weights should be\n",
            "        defined for each class of every column in its own dict. For example,\n",
            "        for four-class multilabel classification weights should be\n",
            "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            "\n",
            "        The \"balanced\" mode uses the values of y to automatically adjust\n",
            "        weights inversely proportional to class frequencies in the input data\n",
            "        as ``n_samples / (n_classes * np.bincount(y))``\n",
            "\n",
            "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
            "        weights are computed based on the bootstrap sample for every tree\n",
            "        grown.\n",
            "\n",
            "        For multi-output, the weights of each column of y will be multiplied.\n",
            "\n",
            "        Note that these weights will be multiplied with sample_weight (passed\n",
            "        through the fit method) if sample_weight is specified.\n",
            "\n",
            "    ccp_alpha : non-negative float, default=0.0\n",
            "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            "        subtree with the largest cost complexity that is smaller than\n",
            "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            "        :ref:`minimal_cost_complexity_pruning` for details.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    max_samples : int or float, default=None\n",
            "        If bootstrap is True, the number of samples to draw from X\n",
            "        to train each base estimator.\n",
            "\n",
            "        - If None (default), then draw `X.shape[0]` samples.\n",
            "        - If int, then draw `max_samples` samples.\n",
            "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
            "        Indicates the monotonicity constraint to enforce on each feature.\n",
            "          - 1: monotonically increasing\n",
            "          - 0: no constraint\n",
            "          - -1: monotonically decreasing\n",
            "\n",
            "        If monotonic_cst is None, no constraints are applied.\n",
            "\n",
            "        Monotonicity constraints are not supported for:\n",
            "          - multiclass classifications (i.e. when `n_classes > 2`),\n",
            "          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
            "          - classifications trained on data with missing values.\n",
            "\n",
            "        The constraints hold over the probability of the positive class.\n",
            "\n",
            "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    estimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`\n",
            "        The child estimator template used to create the collection of fitted\n",
            "        sub-estimators.\n",
            "\n",
            "        .. versionadded:: 1.2\n",
            "           `base_estimator_` was renamed to `estimator_`.\n",
            "\n",
            "    estimators_ : list of DecisionTreeClassifier\n",
            "        The collection of fitted sub-estimators.\n",
            "\n",
            "    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
            "        The classes labels (single output problem), or a list of arrays of\n",
            "        class labels (multi-output problem).\n",
            "\n",
            "    n_classes_ : int or list\n",
            "        The number of classes (single output problem), or a list containing the\n",
            "        number of classes for each output (multi-output problem).\n",
            "\n",
            "    feature_importances_ : ndarray of shape (n_features,)\n",
            "        The impurity-based feature importances.\n",
            "        The higher, the more important the feature.\n",
            "        The importance of a feature is computed as the (normalized)\n",
            "        total reduction of the criterion brought by that feature.  It is also\n",
            "        known as the Gini importance.\n",
            "\n",
            "        Warning: impurity-based feature importances can be misleading for\n",
            "        high cardinality features (many unique values). See\n",
            "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            "\n",
            "    n_features_in_ : int\n",
            "        Number of features seen during :term:`fit`.\n",
            "\n",
            "        .. versionadded:: 0.24\n",
            "\n",
            "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            "        Names of features seen during :term:`fit`. Defined only when `X`\n",
            "        has feature names that are all strings.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "\n",
            "    n_outputs_ : int\n",
            "        The number of outputs when ``fit`` is performed.\n",
            "\n",
            "    oob_score_ : float\n",
            "        Score of the training dataset obtained using an out-of-bag estimate.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\n",
            "            (n_samples, n_classes, n_outputs)\n",
            "        Decision function computed with out-of-bag estimate on the training\n",
            "        set. If n_estimators is small it might be possible that a data point\n",
            "        was never left out during the bootstrap. In this case,\n",
            "        `oob_decision_function_` might contain NaN. This attribute exists\n",
            "        only when ``oob_score`` is True.\n",
            "\n",
            "    estimators_samples_ : list of arrays\n",
            "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
            "        estimator. Each subset is defined by an array of the indices selected.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    ExtraTreesRegressor : An extra-trees regressor with random splits.\n",
            "    RandomForestClassifier : A random forest classifier with optimal splits.\n",
            "    RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The default values for the parameters controlling the size of the trees\n",
            "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            "    unpruned trees which can potentially be very large on some data sets. To\n",
            "    reduce memory consumption, the complexity and size of the trees should be\n",
            "    controlled by setting those parameter values.\n",
            "\n",
            "    References\n",
            "    ----------\n",
            "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
            "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.ensemble import ExtraTreesClassifier\n",
            "    >>> from sklearn.datasets import make_classification\n",
            "    >>> X, y = make_classification(n_features=4, random_state=0)\n",
            "    >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
            "    >>> clf.fit(X, y)\n",
            "    ExtraTreesClassifier(random_state=0)\n",
            "    >>> clf.predict([[0, 0, 0, 0]])\n",
            "    array([1])\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        **ForestClassifier._parameter_constraints,\n",
            "        **DecisionTreeClassifier._parameter_constraints,\n",
            "        \"class_weight\": [\n",
            "            StrOptions({\"balanced_subsample\", \"balanced\"}),\n",
            "            dict,\n",
            "            list,\n",
            "            None,\n",
            "        ],\n",
            "    }\n",
            "    _parameter_constraints.pop(\"splitter\")\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        criterion=\"gini\",\n",
            "        max_depth=None,\n",
            "        min_samples_split=2,\n",
            "        min_samples_leaf=1,\n",
            "        min_weight_fraction_leaf=0.0,\n",
            "        max_features=\"sqrt\",\n",
            "        max_leaf_nodes=None,\n",
            "        min_impurity_decrease=0.0,\n",
            "        bootstrap=False,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        class_weight=None,\n",
            "        ccp_alpha=0.0,\n",
            "        max_samples=None,\n",
            "        monotonic_cst=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=ExtraTreeClassifier(),\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=(\n",
            "                \"criterion\",\n",
            "                \"max_depth\",\n",
            "                \"min_samples_split\",\n",
            "                \"min_samples_leaf\",\n",
            "                \"min_weight_fraction_leaf\",\n",
            "                \"max_features\",\n",
            "                \"max_leaf_nodes\",\n",
            "                \"min_impurity_decrease\",\n",
            "                \"random_state\",\n",
            "                \"ccp_alpha\",\n",
            "                \"monotonic_cst\",\n",
            "            ),\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            class_weight=class_weight,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "        self.criterion = criterion\n",
            "        self.max_depth = max_depth\n",
            "        self.min_samples_split = min_samples_split\n",
            "        self.min_samples_leaf = min_samples_leaf\n",
            "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
            "        self.max_features = max_features\n",
            "        self.max_leaf_nodes = max_leaf_nodes\n",
            "        self.min_impurity_decrease = min_impurity_decrease\n",
            "        self.ccp_alpha = ccp_alpha\n",
            "        self.monotonic_cst = monotonic_cst\n",
            "\n",
            "\n",
            "class ExtraTreesRegressor(ForestRegressor):\n",
            "    \"\"\"\n",
            "    An extra-trees regressor.\n",
            "\n",
            "    This class implements a meta estimator that fits a number of\n",
            "    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
            "    of the dataset and uses averaging to improve the predictive accuracy\n",
            "    and control over-fitting.\n",
            "\n",
            "    Read more in the :ref:`User Guide <forest>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_estimators : int, default=100\n",
            "        The number of trees in the forest.\n",
            "\n",
            "        .. versionchanged:: 0.22\n",
            "           The default value of ``n_estimators`` changed from 10 to 100\n",
            "           in 0.22.\n",
            "\n",
            "    criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\n",
            "            default=\"squared_error\"\n",
            "        The function to measure the quality of a split. Supported criteria\n",
            "        are \"squared_error\" for the mean squared error, which is equal to\n",
            "        variance reduction as feature selection criterion and minimizes the L2\n",
            "        loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
            "        mean squared error with Friedman's improvement score for potential\n",
            "        splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
            "        the L1 loss using the median of each terminal node, and \"poisson\" which\n",
            "        uses reduction in Poisson deviance to find splits.\n",
            "        Training using \"absolute_error\" is significantly slower\n",
            "        than when using \"squared_error\".\n",
            "\n",
            "        .. versionadded:: 0.18\n",
            "           Mean Absolute Error (MAE) criterion.\n",
            "\n",
            "    max_depth : int, default=None\n",
            "        The maximum depth of the tree. If None, then nodes are expanded until\n",
            "        all leaves are pure or until all leaves contain less than\n",
            "        min_samples_split samples.\n",
            "\n",
            "    min_samples_split : int or float, default=2\n",
            "        The minimum number of samples required to split an internal node:\n",
            "\n",
            "        - If int, then consider `min_samples_split` as the minimum number.\n",
            "        - If float, then `min_samples_split` is a fraction and\n",
            "          `ceil(min_samples_split * n_samples)` are the minimum\n",
            "          number of samples for each split.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_samples_leaf : int or float, default=1\n",
            "        The minimum number of samples required to be at a leaf node.\n",
            "        A split point at any depth will only be considered if it leaves at\n",
            "        least ``min_samples_leaf`` training samples in each of the left and\n",
            "        right branches.  This may have the effect of smoothing the model,\n",
            "        especially in regression.\n",
            "\n",
            "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
            "        - If float, then `min_samples_leaf` is a fraction and\n",
            "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            "          number of samples for each node.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_weight_fraction_leaf : float, default=0.0\n",
            "        The minimum weighted fraction of the sum total of weights (of all\n",
            "        the input samples) required to be at a leaf node. Samples have\n",
            "        equal weight when sample_weight is not provided.\n",
            "\n",
            "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
            "        The number of features to consider when looking for the best split:\n",
            "\n",
            "        - If int, then consider `max_features` features at each split.\n",
            "        - If float, then `max_features` is a fraction and\n",
            "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
            "          split.\n",
            "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            "        - If \"log2\", then `max_features=log2(n_features)`.\n",
            "        - If None or 1.0, then `max_features=n_features`.\n",
            "\n",
            "        .. note::\n",
            "            The default of 1.0 is equivalent to bagged trees and more\n",
            "            randomness can be achieved by setting smaller values, e.g. 0.3.\n",
            "\n",
            "        .. versionchanged:: 1.1\n",
            "            The default of `max_features` changed from `\"auto\"` to 1.0.\n",
            "\n",
            "        Note: the search for a split does not stop until at least one\n",
            "        valid partition of the node samples is found, even if it requires to\n",
            "        effectively inspect more than ``max_features`` features.\n",
            "\n",
            "    max_leaf_nodes : int, default=None\n",
            "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            "        Best nodes are defined as relative reduction in impurity.\n",
            "        If None then unlimited number of leaf nodes.\n",
            "\n",
            "    min_impurity_decrease : float, default=0.0\n",
            "        A node will be split if this split induces a decrease of the impurity\n",
            "        greater than or equal to this value.\n",
            "\n",
            "        The weighted impurity decrease equation is the following::\n",
            "\n",
            "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            "                                - N_t_L / N_t * left_impurity)\n",
            "\n",
            "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
            "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
            "\n",
            "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            "        if ``sample_weight`` is passed.\n",
            "\n",
            "        .. versionadded:: 0.19\n",
            "\n",
            "    bootstrap : bool, default=False\n",
            "        Whether bootstrap samples are used when building trees. If False, the\n",
            "        whole dataset is used to build each tree.\n",
            "\n",
            "    oob_score : bool or callable, default=False\n",
            "        Whether to use out-of-bag samples to estimate the generalization score.\n",
            "        By default, :func:`~sklearn.metrics.r2_score` is used.\n",
            "        Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
            "        custom metric. Only available if `bootstrap=True`.\n",
            "\n",
            "    n_jobs : int, default=None\n",
            "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            "        context. ``-1`` means using all processors. See :term:`Glossary\n",
            "        <n_jobs>` for more details.\n",
            "\n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Controls 3 sources of randomness:\n",
            "\n",
            "        - the bootstrapping of the samples used when building trees\n",
            "          (if ``bootstrap=True``)\n",
            "        - the sampling of the features to consider when looking for the best\n",
            "          split at each node (if ``max_features < n_features``)\n",
            "        - the draw of the splits for each of the `max_features`\n",
            "\n",
            "        See :term:`Glossary <random_state>` for details.\n",
            "\n",
            "    verbose : int, default=0\n",
            "        Controls the verbosity when fitting and predicting.\n",
            "\n",
            "    warm_start : bool, default=False\n",
            "        When set to ``True``, reuse the solution of the previous call to fit\n",
            "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
            "        new forest. See :term:`Glossary <warm_start>` and\n",
            "        :ref:`tree_ensemble_warm_start` for details.\n",
            "\n",
            "    ccp_alpha : non-negative float, default=0.0\n",
            "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            "        subtree with the largest cost complexity that is smaller than\n",
            "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            "        :ref:`minimal_cost_complexity_pruning` for details.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    max_samples : int or float, default=None\n",
            "        If bootstrap is True, the number of samples to draw from X\n",
            "        to train each base estimator.\n",
            "\n",
            "        - If None (default), then draw `X.shape[0]` samples.\n",
            "        - If int, then draw `max_samples` samples.\n",
            "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
            "\n",
            "        .. versionadded:: 0.22\n",
            "\n",
            "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
            "        Indicates the monotonicity constraint to enforce on each feature.\n",
            "          - 1: monotonically increasing\n",
            "          - 0: no constraint\n",
            "          - -1: monotonically decreasing\n",
            "\n",
            "        If monotonic_cst is None, no constraints are applied.\n",
            "\n",
            "        Monotonicity constraints are not supported for:\n",
            "          - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
            "          - regressions trained on data with missing values.\n",
            "\n",
            "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n",
            "        The child estimator template used to create the collection of fitted\n",
            "        sub-estimators.\n",
            "\n",
            "        .. versionadded:: 1.2\n",
            "           `base_estimator_` was renamed to `estimator_`.\n",
            "\n",
            "    estimators_ : list of DecisionTreeRegressor\n",
            "        The collection of fitted sub-estimators.\n",
            "\n",
            "    feature_importances_ : ndarray of shape (n_features,)\n",
            "        The impurity-based feature importances.\n",
            "        The higher, the more important the feature.\n",
            "        The importance of a feature is computed as the (normalized)\n",
            "        total reduction of the criterion brought by that feature.  It is also\n",
            "        known as the Gini importance.\n",
            "\n",
            "        Warning: impurity-based feature importances can be misleading for\n",
            "        high cardinality features (many unique values). See\n",
            "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            "\n",
            "    n_features_in_ : int\n",
            "        Number of features seen during :term:`fit`.\n",
            "\n",
            "        .. versionadded:: 0.24\n",
            "\n",
            "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            "        Names of features seen during :term:`fit`. Defined only when `X`\n",
            "        has feature names that are all strings.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "\n",
            "    n_outputs_ : int\n",
            "        The number of outputs.\n",
            "\n",
            "    oob_score_ : float\n",
            "        Score of the training dataset obtained using an out-of-bag estimate.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
            "        Prediction computed with out-of-bag estimate on the training set.\n",
            "        This attribute exists only when ``oob_score`` is True.\n",
            "\n",
            "    estimators_samples_ : list of arrays\n",
            "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
            "        estimator. Each subset is defined by an array of the indices selected.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    ExtraTreesClassifier : An extra-trees classifier with random splits.\n",
            "    RandomForestClassifier : A random forest classifier with optimal splits.\n",
            "    RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The default values for the parameters controlling the size of the trees\n",
            "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            "    unpruned trees which can potentially be very large on some data sets. To\n",
            "    reduce memory consumption, the complexity and size of the trees should be\n",
            "    controlled by setting those parameter values.\n",
            "\n",
            "    References\n",
            "    ----------\n",
            "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
            "           Machine Learning, 63(1), 3-42, 2006.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.datasets import load_diabetes\n",
            "    >>> from sklearn.model_selection import train_test_split\n",
            "    >>> from sklearn.ensemble import ExtraTreesRegressor\n",
            "    >>> X, y = load_diabetes(return_X_y=True)\n",
            "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
            "    ...     X, y, random_state=0)\n",
            "    >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n",
            "    ...    X_train, y_train)\n",
            "    >>> reg.score(X_test, y_test)\n",
            "    0.2727...\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        **ForestRegressor._parameter_constraints,\n",
            "        **DecisionTreeRegressor._parameter_constraints,\n",
            "    }\n",
            "    _parameter_constraints.pop(\"splitter\")\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        criterion=\"squared_error\",\n",
            "        max_depth=None,\n",
            "        min_samples_split=2,\n",
            "        min_samples_leaf=1,\n",
            "        min_weight_fraction_leaf=0.0,\n",
            "        max_features=1.0,\n",
            "        max_leaf_nodes=None,\n",
            "        min_impurity_decrease=0.0,\n",
            "        bootstrap=False,\n",
            "        oob_score=False,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "        ccp_alpha=0.0,\n",
            "        max_samples=None,\n",
            "        monotonic_cst=None,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=ExtraTreeRegressor(),\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=(\n",
            "                \"criterion\",\n",
            "                \"max_depth\",\n",
            "                \"min_samples_split\",\n",
            "                \"min_samples_leaf\",\n",
            "                \"min_weight_fraction_leaf\",\n",
            "                \"max_features\",\n",
            "                \"max_leaf_nodes\",\n",
            "                \"min_impurity_decrease\",\n",
            "                \"random_state\",\n",
            "                \"ccp_alpha\",\n",
            "                \"monotonic_cst\",\n",
            "            ),\n",
            "            bootstrap=bootstrap,\n",
            "            oob_score=oob_score,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            max_samples=max_samples,\n",
            "        )\n",
            "\n",
            "        self.criterion = criterion\n",
            "        self.max_depth = max_depth\n",
            "        self.min_samples_split = min_samples_split\n",
            "        self.min_samples_leaf = min_samples_leaf\n",
            "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
            "        self.max_features = max_features\n",
            "        self.max_leaf_nodes = max_leaf_nodes\n",
            "        self.min_impurity_decrease = min_impurity_decrease\n",
            "        self.ccp_alpha = ccp_alpha\n",
            "        self.monotonic_cst = monotonic_cst\n",
            "\n",
            "\n",
            "class RandomTreesEmbedding(TransformerMixin, BaseForest):\n",
            "    \"\"\"\n",
            "    An ensemble of totally random trees.\n",
            "\n",
            "    An unsupervised transformation of a dataset to a high-dimensional\n",
            "    sparse representation. A datapoint is coded according to which leaf of\n",
            "    each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
            "    this leads to a binary coding with as many ones as there are trees in\n",
            "    the forest.\n",
            "\n",
            "    The dimensionality of the resulting representation is\n",
            "    ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n",
            "    the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n",
            "\n",
            "    Read more in the :ref:`User Guide <random_trees_embedding>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    n_estimators : int, default=100\n",
            "        Number of trees in the forest.\n",
            "\n",
            "        .. versionchanged:: 0.22\n",
            "           The default value of ``n_estimators`` changed from 10 to 100\n",
            "           in 0.22.\n",
            "\n",
            "    max_depth : int, default=5\n",
            "        The maximum depth of each tree. If None, then nodes are expanded until\n",
            "        all leaves are pure or until all leaves contain less than\n",
            "        min_samples_split samples.\n",
            "\n",
            "    min_samples_split : int or float, default=2\n",
            "        The minimum number of samples required to split an internal node:\n",
            "\n",
            "        - If int, then consider `min_samples_split` as the minimum number.\n",
            "        - If float, then `min_samples_split` is a fraction and\n",
            "          `ceil(min_samples_split * n_samples)` is the minimum\n",
            "          number of samples for each split.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_samples_leaf : int or float, default=1\n",
            "        The minimum number of samples required to be at a leaf node.\n",
            "        A split point at any depth will only be considered if it leaves at\n",
            "        least ``min_samples_leaf`` training samples in each of the left and\n",
            "        right branches.  This may have the effect of smoothing the model,\n",
            "        especially in regression.\n",
            "\n",
            "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
            "        - If float, then `min_samples_leaf` is a fraction and\n",
            "          `ceil(min_samples_leaf * n_samples)` is the minimum\n",
            "          number of samples for each node.\n",
            "\n",
            "        .. versionchanged:: 0.18\n",
            "           Added float values for fractions.\n",
            "\n",
            "    min_weight_fraction_leaf : float, default=0.0\n",
            "        The minimum weighted fraction of the sum total of weights (of all\n",
            "        the input samples) required to be at a leaf node. Samples have\n",
            "        equal weight when sample_weight is not provided.\n",
            "\n",
            "    max_leaf_nodes : int, default=None\n",
            "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            "        Best nodes are defined as relative reduction in impurity.\n",
            "        If None then unlimited number of leaf nodes.\n",
            "\n",
            "    min_impurity_decrease : float, default=0.0\n",
            "        A node will be split if this split induces a decrease of the impurity\n",
            "        greater than or equal to this value.\n",
            "\n",
            "        The weighted impurity decrease equation is the following::\n",
            "\n",
            "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            "                                - N_t_L / N_t * left_impurity)\n",
            "\n",
            "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
            "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
            "\n",
            "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            "        if ``sample_weight`` is passed.\n",
            "\n",
            "        .. versionadded:: 0.19\n",
            "\n",
            "    sparse_output : bool, default=True\n",
            "        Whether or not to return a sparse CSR matrix, as default behavior,\n",
            "        or to return a dense array compatible with dense pipeline operators.\n",
            "\n",
            "    n_jobs : int, default=None\n",
            "        The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n",
            "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            "        context. ``-1`` means using all processors. See :term:`Glossary\n",
            "        <n_jobs>` for more details.\n",
            "\n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Controls the generation of the random `y` used to fit the trees\n",
            "        and the draw of the splits for each feature at the trees' nodes.\n",
            "        See :term:`Glossary <random_state>` for details.\n",
            "\n",
            "    verbose : int, default=0\n",
            "        Controls the verbosity when fitting and predicting.\n",
            "\n",
            "    warm_start : bool, default=False\n",
            "        When set to ``True``, reuse the solution of the previous call to fit\n",
            "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
            "        new forest. See :term:`Glossary <warm_start>` and\n",
            "        :ref:`tree_ensemble_warm_start` for details.\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n",
            "        The child estimator template used to create the collection of fitted\n",
            "        sub-estimators.\n",
            "\n",
            "        .. versionadded:: 1.2\n",
            "           `base_estimator_` was renamed to `estimator_`.\n",
            "\n",
            "    estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances\n",
            "        The collection of fitted sub-estimators.\n",
            "\n",
            "    feature_importances_ : ndarray of shape (n_features,)\n",
            "        The feature importances (the higher, the more important the feature).\n",
            "\n",
            "    n_features_in_ : int\n",
            "        Number of features seen during :term:`fit`.\n",
            "\n",
            "        .. versionadded:: 0.24\n",
            "\n",
            "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            "        Names of features seen during :term:`fit`. Defined only when `X`\n",
            "        has feature names that are all strings.\n",
            "\n",
            "        .. versionadded:: 1.0\n",
            "\n",
            "    n_outputs_ : int\n",
            "        The number of outputs when ``fit`` is performed.\n",
            "\n",
            "    one_hot_encoder_ : OneHotEncoder instance\n",
            "        One-hot encoder used to create the sparse embedding.\n",
            "\n",
            "    estimators_samples_ : list of arrays\n",
            "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
            "        estimator. Each subset is defined by an array of the indices selected.\n",
            "\n",
            "        .. versionadded:: 1.4\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    ExtraTreesClassifier : An extra-trees classifier.\n",
            "    ExtraTreesRegressor : An extra-trees regressor.\n",
            "    RandomForestClassifier : A random forest classifier.\n",
            "    RandomForestRegressor : A random forest regressor.\n",
            "    sklearn.tree.ExtraTreeClassifier: An extremely randomized\n",
            "        tree classifier.\n",
            "    sklearn.tree.ExtraTreeRegressor : An extremely randomized\n",
            "        tree regressor.\n",
            "\n",
            "    References\n",
            "    ----------\n",
            "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
            "           Machine Learning, 63(1), 3-42, 2006.\n",
            "    .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
            "           visual codebooks using randomized clustering forests\"\n",
            "           NIPS 2007\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.ensemble import RandomTreesEmbedding\n",
            "    >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n",
            "    >>> random_trees = RandomTreesEmbedding(\n",
            "    ...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n",
            "    >>> X_sparse_embedding = random_trees.transform(X)\n",
            "    >>> X_sparse_embedding.toarray()\n",
            "    array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
            "           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
            "           [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
            "           [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
            "           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])\n",
            "    \"\"\"\n",
            "\n",
            "    _parameter_constraints: dict = {\n",
            "        \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n",
            "        \"n_jobs\": [Integral, None],\n",
            "        \"verbose\": [\"verbose\"],\n",
            "        \"warm_start\": [\"boolean\"],\n",
            "        **BaseDecisionTree._parameter_constraints,\n",
            "        \"sparse_output\": [\"boolean\"],\n",
            "    }\n",
            "    for param in (\"max_features\", \"ccp_alpha\", \"splitter\", \"monotonic_cst\"):\n",
            "        _parameter_constraints.pop(param)\n",
            "\n",
            "    criterion = \"squared_error\"\n",
            "    max_features = 1\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        n_estimators=100,\n",
            "        *,\n",
            "        max_depth=5,\n",
            "        min_samples_split=2,\n",
            "        min_samples_leaf=1,\n",
            "        min_weight_fraction_leaf=0.0,\n",
            "        max_leaf_nodes=None,\n",
            "        min_impurity_decrease=0.0,\n",
            "        sparse_output=True,\n",
            "        n_jobs=None,\n",
            "        random_state=None,\n",
            "        verbose=0,\n",
            "        warm_start=False,\n",
            "    ):\n",
            "        super().__init__(\n",
            "            estimator=ExtraTreeRegressor(),\n",
            "            n_estimators=n_estimators,\n",
            "            estimator_params=(\n",
            "                \"criterion\",\n",
            "                \"max_depth\",\n",
            "                \"min_samples_split\",\n",
            "                \"min_samples_leaf\",\n",
            "                \"min_weight_fraction_leaf\",\n",
            "                \"max_features\",\n",
            "                \"max_leaf_nodes\",\n",
            "                \"min_impurity_decrease\",\n",
            "                \"random_state\",\n",
            "            ),\n",
            "            bootstrap=False,\n",
            "            oob_score=False,\n",
            "            n_jobs=n_jobs,\n",
            "            random_state=random_state,\n",
            "            verbose=verbose,\n",
            "            warm_start=warm_start,\n",
            "            max_samples=None,\n",
            "        )\n",
            "\n",
            "        self.max_depth = max_depth\n",
            "        self.min_samples_split = min_samples_split\n",
            "        self.min_samples_leaf = min_samples_leaf\n",
            "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
            "        self.max_leaf_nodes = max_leaf_nodes\n",
            "        self.min_impurity_decrease = min_impurity_decrease\n",
            "        self.sparse_output = sparse_output\n",
            "\n",
            "    def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n",
            "        raise NotImplementedError(\"OOB score not supported by tree embedding\")\n",
            "\n",
            "    def fit(self, X, y=None, sample_weight=None):\n",
            "        \"\"\"\n",
            "        Fit estimator.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            The input samples. Use ``dtype=np.float32`` for maximum\n",
            "            efficiency. Sparse matrices are also supported, use sparse\n",
            "            ``csc_matrix`` for maximum efficiency.\n",
            "\n",
            "        y : Ignored\n",
            "            Not used, present for API consistency by convention.\n",
            "\n",
            "        sample_weight : array-like of shape (n_samples,), default=None\n",
            "            Sample weights. If None, then samples are equally weighted. Splits\n",
            "            that would create child nodes with net zero or negative weight are\n",
            "            ignored while searching for a split in each node. In the case of\n",
            "            classification, splits are also ignored if they would result in any\n",
            "            single class carrying a negative weight in either child node.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        self : object\n",
            "            Returns the instance itself.\n",
            "        \"\"\"\n",
            "        # Parameters are validated in fit_transform\n",
            "        self.fit_transform(X, y, sample_weight=sample_weight)\n",
            "        return self\n",
            "\n",
            "    @_fit_context(prefer_skip_nested_validation=True)\n",
            "    def fit_transform(self, X, y=None, sample_weight=None):\n",
            "        \"\"\"\n",
            "        Fit estimator and transform dataset.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            Input data used to build forests. Use ``dtype=np.float32`` for\n",
            "            maximum efficiency.\n",
            "\n",
            "        y : Ignored\n",
            "            Not used, present for API consistency by convention.\n",
            "\n",
            "        sample_weight : array-like of shape (n_samples,), default=None\n",
            "            Sample weights. If None, then samples are equally weighted. Splits\n",
            "            that would create child nodes with net zero or negative weight are\n",
            "            ignored while searching for a split in each node. In the case of\n",
            "            classification, splits are also ignored if they would result in any\n",
            "            single class carrying a negative weight in either child node.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X_transformed : sparse matrix of shape (n_samples, n_out)\n",
            "            Transformed dataset.\n",
            "        \"\"\"\n",
            "        rnd = check_random_state(self.random_state)\n",
            "        y = rnd.uniform(size=_num_samples(X))\n",
            "        super().fit(X, y, sample_weight=sample_weight)\n",
            "\n",
            "        self.one_hot_encoder_ = OneHotEncoder(sparse_output=self.sparse_output)\n",
            "        output = self.one_hot_encoder_.fit_transform(self.apply(X))\n",
            "        self._n_features_out = output.shape[1]\n",
            "        return output\n",
            "\n",
            "    def get_feature_names_out(self, input_features=None):\n",
            "        \"\"\"Get output feature names for transformation.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        input_features : array-like of str or None, default=None\n",
            "            Only used to validate feature names with the names seen in :meth:`fit`.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        feature_names_out : ndarray of str objects\n",
            "            Transformed feature names, in the format of\n",
            "            `randomtreesembedding_{tree}_{leaf}`, where `tree` is the tree used\n",
            "            to generate the leaf and `leaf` is the index of a leaf node\n",
            "            in that tree. Note that the node indexing scheme is used to\n",
            "            index both nodes with children (split nodes) and leaf nodes.\n",
            "            Only the latter can be present as output features.\n",
            "            As a consequence, there are missing indices in the output\n",
            "            feature names.\n",
            "        \"\"\"\n",
            "        check_is_fitted(self, \"_n_features_out\")\n",
            "        _check_feature_names_in(\n",
            "            self, input_features=input_features, generate_names=False\n",
            "        )\n",
            "\n",
            "        feature_names = [\n",
            "            f\"randomtreesembedding_{tree}_{leaf}\"\n",
            "            for tree in range(self.n_estimators)\n",
            "            for leaf in self.one_hot_encoder_.categories_[tree]\n",
            "        ]\n",
            "        return np.asarray(feature_names, dtype=object)\n",
            "\n",
            "    def transform(self, X):\n",
            "        \"\"\"\n",
            "        Transform dataset.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            Input data to be transformed. Use ``dtype=np.float32`` for maximum\n",
            "            efficiency. Sparse matrices are also supported, use sparse\n",
            "            ``csr_matrix`` for maximum efficiency.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X_transformed : sparse matrix of shape (n_samples, n_out)\n",
            "            Transformed dataset.\n",
            "        \"\"\"\n",
            "        check_is_fitted(self)\n",
            "        return self.one_hot_encoder_.transform(self.apply(X))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNcLsW4pI19V",
        "outputId": "0bc01842-587d-4178-a8b5-79835d6b64fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[2]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# 載入數據\n",
        "data = {\n",
        "    'Pregnancies': [0, 8, 7, 9, 1, 0, 0],\n",
        "    'PlasmaGlucose': [171, 92, 115, 103, 85, 82, 133],\n",
        "    'DiastolicBloodPressure': [80, 93, 47, 78, 59, 92, 47],\n",
        "    'TricepsThickness': [34, 47, 52, 25, 27, 9, 19],\n",
        "    'SerumInsulin': [23, 36, 35, 304, 35, 253, 227],\n",
        "    'BMI': [43.50972593, 21.24057571, 41.51152348, 29.58219193, 42.60453585, 19.72416021, 21.94135672],\n",
        "    'DiabetesPedigree': [1.213191354, 0.158364981, 0.079018568, 1.282869847, 0.549541871, 0.103424498, 0.174159779],\n",
        "    'Age': [21, 23, 23, 43, 22, 26, 21],\n",
        "    'Diabetic': [0, 0, 0, 1, 0, 0, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 分割特徵和標籤\n",
        "X = df.drop('Diabetic', axis=1)\n",
        "y = df['Diabetic']\n",
        "\n",
        "# 分割數據為訓練和測試集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 訓練隨機森林分類器\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 預測測試集\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 評估模型\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    }
  ]
}